{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification Models for AI-Generated Code Detection\n",
        "\n",
        "Train and evaluate multiple classifiers (Logistic Regression, Decision Tree, Random Forest, MLP, LinearSVC) on semantic features extracted from code snippets. Includes threshold tuning, ensemble voting, rule-based post-processing, and final submission generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contents\n",
        "1. Imports & Data Loading\n",
        "2. Feature Preparation & Train/Val Split\n",
        "3. Logistic Regression\n",
        "4. Decision Tree\n",
        "5. Random Forest\n",
        "6. MLP\n",
        "7. LinearSVC\n",
        "8. Combined Vectorizer Approach\n",
        "9. Only Old Features Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DNzGpt7pVGdW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "drive_path = \"/data/semeval\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in .\\.venv\\Lib\\site-packages (3.0.0)\n",
            "Requirement already satisfied: numpy>=2.3.3 in .\\.venv\\Lib\\site-packages (from pandas) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U pandas\n",
        "data_path_train = \"data\\\\semeval\\\\train.parquet\"\n",
        "data_path_validation = \"data\\\\semeval\\\\validation.parquet\"\n",
        "# data_path_test = (base_dir / \"test.parquet\").as_posix()\n",
        "\n",
        "df_train = pd.read_parquet(data_path_train)\n",
        "df_validation = pd.read_parquet(data_path_validation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive_path = \"/data/semeval/processed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ce7VJbloVa4F"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "<>:4: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "<>:1: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "<>:4: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_42908\\1009075286.py:1: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "  data_path_feats_200k = \"data\\semeval\\processed\\\\test_feats_2.csv\"\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_42908\\1009075286.py:4: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n",
            "  data_path_feats_300k = \"data\\semeval\\processed\\\\test_feats.csv\"\n"
          ]
        }
      ],
      "source": [
        "data_path_feats_200k = \"data\\semeval\\processed\\\\test_feats_2.csv\"\n",
        "df_feats_200k = pd.read_csv(data_path_feats_200k)\n",
        "\n",
        "data_path_feats_300k = \"data\\semeval\\processed\\\\test_feats.csv\"\n",
        "df_feats_300k = pd.read_csv(data_path_feats_300k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_path_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_test = pd.read_parquet(\u001b[43mdata_path_test\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'data_path_test' is not defined"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_parquet(data_path_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JttR9TFEV28z"
      },
      "outputs": [],
      "source": [
        "X_test = []\n",
        "for index, row in df_feats_300k.iterrows():\n",
        "  X_test.append(list(row.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RKwbywpKWoDQ"
      },
      "outputs": [],
      "source": [
        "for index, row in df_feats_200k.iterrows():\n",
        "  X_test.append(list(row.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V27KSxpZCkAL"
      },
      "outputs": [],
      "source": [
        "data_path_feats_train = \"data\\\\semeval\\\\processed\\\\train_feats_300k.csv\"\n",
        "df_feats_train = pd.read_csv(data_path_feats_train)\n",
        "\n",
        "data_path_feats_val = \"data\\\\semeval\\\\processed\\\\val_feats.csv\"\n",
        "df_feats_val = pd.read_csv(data_path_feats_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YIipoFxsFY49"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "for index, row in df_feats_train.iterrows():\n",
        "  X_train.append(list(row.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wRh8cB_pGC3T"
      },
      "outputs": [],
      "source": [
        "Y_train = []\n",
        "for index, row in df_train.head(300000).iterrows():\n",
        "  Y_train.append(row['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vQ1Etw4BFdtu"
      },
      "outputs": [],
      "source": [
        "X_val = []\n",
        "for index, row in df_feats_val.iterrows():\n",
        "  X_val.append(list(row.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PDicsyVmGLo3"
      },
      "outputs": [],
      "source": [
        "Y_val = []\n",
        "for index, row in df_validation.iterrows():\n",
        "  if row[\"language\"] != \"Python\":\n",
        "    Y_val.append(row['label'])\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "twqw_wcJFmIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HH_9DoY0GzjJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a6yyyW6tG3MI"
      },
      "outputs": [],
      "source": [
        "feature_names = [\n",
        "        \"verb_ratio_comments\",\n",
        "        \"text_like_ratio\",\n",
        "        # \"code_like_ratio\",\n",
        "        \"comments_code_like_ratio_to_total\",\n",
        "        \"comments_text_like_ratio_to_total\",\n",
        "        \"comments_code_like_ratio_comments\",\n",
        "        \"comments_text_like_ratio_comments\",\n",
        "        # \"identifiers_verb_ratio:\",\n",
        "        # \"cyclomatic_complexity_mean\",\n",
        "        # \"cyclomatic_complexity_std\",\n",
        "        # \"cyclomatic_complexity_max\",\n",
        "        \"error_near_eof_ratio\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RtumFhNGAbIB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "COMMENT_RE = re.compile(r\"(#|//|/\\*|\\*/)\")\n",
        "\n",
        "def get_comment_ratio(code: str) -> dict:\n",
        "    lines = code.splitlines()\n",
        "    loc = len(lines)\n",
        "\n",
        "    # ---- comment structure ----\n",
        "    comment_lines = 0\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if COMMENT_RE.search(line) and \"@\" not in line:\n",
        "            comment_lines += 1\n",
        "\n",
        "            if line.strip().startswith((\"#\", \"//\")) is False :\n",
        "                comment_lines -= 1\n",
        "\n",
        "    return comment_lines if loc == 0 else comment_lines / loc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ezYHrGmHnhYO"
      },
      "outputs": [],
      "source": [
        "X_comments_train = []\n",
        "for index, row in df_train.head(300000).iterrows():\n",
        "  code = row['code']\n",
        "  X_comments_train.append(get_comment_ratio(code))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cZ_Qg1MuXa5X"
      },
      "outputs": [],
      "source": [
        "X_train_line = []\n",
        "i = 0\n",
        "for index, row in df_train.head(300000).iterrows():\n",
        "  code = row['code']\n",
        "  loc = len(code.splitlines())\n",
        "  bucket = \"small\"\n",
        "  if loc >= 20 and loc <= 70:\n",
        "    bucket = \"medium\"\n",
        "  elif loc > 70:\n",
        "    bucket = \"large\"\n",
        "  # x_entropy = X_line_entropy_train[i]\n",
        "\n",
        "  comm_h = X_comments_train[i]\n",
        "  X_train_line.append([bucket, comm_h] + X_train[i][1:2])\n",
        "  i = i + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yprxWCcMnuev"
      },
      "outputs": [],
      "source": [
        "X_val_line = []\n",
        "i = 0\n",
        "for index, row in df_validation.iterrows():\n",
        "  if row[\"language\"] != \"Python\":\n",
        "    code = row['code']\n",
        "    bucket = \"small\"\n",
        "    if len(code.splitlines()) >= 20 and len(code.splitlines()) <= 70:\n",
        "      bucket = \"medium\"\n",
        "    elif len(code.splitlines()) > 70:\n",
        "      bucket = \"large\"\n",
        "\n",
        "    comm_h = get_comment_ratio(code)\n",
        "    X_val_line.append([bucket, comm_h] + X_val[i][1:2])\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kWhTmhclpIlZ"
      },
      "outputs": [],
      "source": [
        "bucket_scalers = {}\n",
        "X_train_np = np.asarray(X_train_line, dtype=object)   # keep bucket strings\n",
        "X_train_scaled = np.zeros((X_train_np.shape[0], X_train_np.shape[1] - 1), dtype=np.float32)\n",
        "\n",
        "for bucket in [\"small\", \"medium\", \"large\"]:\n",
        "    idx = [i for i, x in enumerate(X_train_line) if x[0] == bucket]\n",
        "    if not idx:\n",
        "        continue\n",
        "\n",
        "    X_bucket = np.array([X_train_line[i][1:] for i in idx], dtype=np.float32)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_bucket)\n",
        "\n",
        "    # explicit numerical safety\n",
        "    scaler.scale_ = np.where(scaler.scale_ == 0, 1.0, scaler.scale_)\n",
        "\n",
        "    bucket_scalers[bucket] = scaler\n",
        "\n",
        "    X_bucket_scaled = scaler.transform(X_bucket)\n",
        "\n",
        "    for j, i in enumerate(idx):\n",
        "        X_train_scaled[i, :] = X_bucket_scaled[j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9CaphuMpQNX",
        "outputId": "b6338b17-e5f1-4790-8e65-cbf920eb7ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "X_val_np = np.asarray(X_val_line, dtype=object)\n",
        "Y_val = np.asarray(Y_val)\n",
        "\n",
        "print(X_val_np.shape[1])\n",
        "\n",
        "X_val_scaled = np.zeros((X_val_np.shape[0], X_val_np.shape[1] - 1), dtype=np.float32)\n",
        "\n",
        "for i, x in enumerate(X_val_line):\n",
        "    bucket = x[0]\n",
        "    feats = np.asarray(x[1:], dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    if bucket in bucket_scalers:\n",
        "        X_val_scaled[i] = bucket_scalers[bucket].transform(feats)[0]\n",
        "    else:\n",
        "        # fallback: no scaling (or use global_scaler if you have one)\n",
        "        X_val_scaled[i] = feats[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yanTe1Pq5aAc"
      },
      "outputs": [],
      "source": [
        "feature_names = [\n",
        "        \"comment_h\",\n",
        "        \"verb_ratio_comments\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLy0cSG72pN1"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-bRO1EHnpfg",
        "outputId": "9f6b48d7-fa16-4d84-ee4a-393400b2f952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.535905020104348\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.63      0.57      4075\n",
            "           1       0.57      0.45      0.50      4464\n",
            "\n",
            "    accuracy                           0.54      8539\n",
            "   macro avg       0.54      0.54      0.54      8539\n",
            "weighted avg       0.54      0.54      0.53      8539\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
            "  warnings.warn(\n",
            "d:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class SurfaceFeatureTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        rows = []\n",
        "        for code_embedding in X:\n",
        "            rows.append(code_embedding)\n",
        "        return np.asarray(rows, dtype=np.float32)\n",
        "\n",
        "\n",
        "pipe_small = Pipeline([\n",
        "    (\"surface\", SurfaceFeatureTransformer(feature_names)),\n",
        "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "    # (\"scale\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        solver=\"liblinear\",\n",
        "        penalty=\"l1\",\n",
        "        C=0.01,\n",
        "        #class_weight=\"balanced\",\n",
        "        max_iter=5000,\n",
        "        random_state=42\n",
        "    )),\n",
        "])\n",
        "\n",
        "pipe_small.fit(X_train_scaled, Y_train)\n",
        "\n",
        "p1 = pipe_small.predict(X_val_scaled)\n",
        "f1_logreg = f1_score(Y_val, p1, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_val, p1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycri-WUFqgd7",
        "outputId": "46ec3d02-7ac4-44d5-ad9c-9e91eec68842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "  feature  coefficient  abs_coefficient\n",
            "0  comm_h     1.171326         1.171326\n",
            "1    verb     0.539016         0.539016\n"
          ]
        }
      ],
      "source": [
        "clf = pipe_small.named_steps[\"clf\"]\n",
        "coef = clf.coef_[0]   # binary classification\n",
        "\n",
        "feature_names_coef = [\"comm_h\", \"verb\"]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"feature\": feature_names_coef,\n",
        "    \"coefficient\": coef,\n",
        "    \"abs_coefficient\": abs(coef)\n",
        "}).sort_values(\"abs_coefficient\", ascending=False)\n",
        "\n",
        "feature_importance   # display table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHisYnfhPmqu",
        "outputId": "f7d2f5e2-4716-4729-f503-7d71827138a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best threshold: 0.47\n"
          ]
        }
      ],
      "source": [
        "probs = clf.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "thresholds = np.linspace(0.1, 0.9, 81)\n",
        "scores = [\n",
        "    f1_score(Y_val, probs >= t, average=\"macro\")\n",
        "    for t in thresholds\n",
        "]\n",
        "\n",
        "best_t = thresholds[np.argmax(scores)]\n",
        "print(f\"Best threshold: {best_t}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlxbQdoPP1Mu",
        "outputId": "c60cb070-bfe1-413d-8a66-08f279ca9d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5471945243361732\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.49      0.51      4075\n",
            "           1       0.57      0.61      0.58      4464\n",
            "\n",
            "    accuracy                           0.55      8539\n",
            "   macro avg       0.55      0.55      0.55      8539\n",
            "weighted avg       0.55      0.55      0.55      8539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = (clf.predict_proba(X_val_scaled)[:, 1] >= best_t)\n",
        "print(f1_score(Y_val, y_pred, average='macro'))\n",
        "print(classification_report(Y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrwCcIpU2xKo"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SoVbLeTn_lr",
        "outputId": "9be297eb-2a99-4176-f8e5-e93eaacfa993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25323266 -0.21132326]\n",
            "F1_logreg: 0.5411569976785606\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.64      0.57      4075\n",
            "           1       0.58      0.45      0.51      4464\n",
            "\n",
            "    accuracy                           0.54      8539\n",
            "   macro avg       0.55      0.55      0.54      8539\n",
            "weighted avg       0.55      0.54      0.54      8539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipe_tree = Pipeline([\n",
        "    (\"surface\", SurfaceFeatureTransformer(feature_names)),\n",
        "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"clf\", DecisionTreeClassifier(\n",
        "        max_depth=2,\n",
        "        random_state=42\n",
        "    )),\n",
        "])\n",
        "\n",
        "pipe_tree.fit(X_train_scaled, Y_train)\n",
        "p1 = pipe_tree.predict(X_val_scaled)\n",
        "f1_tree = f1_score(Y_val, p1, average='macro')\n",
        "print(f\"F1_tree: {f1_tree}\")\n",
        "print(classification_report(Y_val, p1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl9FYmzd2217"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuGr_KuC2VFE",
        "outputId": "81adea5b-d5ee-4da9-91bb-4d38a8692dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25323266 -0.21132326]\n",
            "F1_forest: 0.5240032699948475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.53      0.51      4075\n",
            "           1       0.55      0.52      0.53      4464\n",
            "\n",
            "    accuracy                           0.52      8539\n",
            "   macro avg       0.52      0.52      0.52      8539\n",
            "weighted avg       0.53      0.52      0.52      8539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipe_forest = Pipeline([\n",
        "    (\"surface\", SurfaceFeatureTransformer(feature_names)),\n",
        "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        max_depth=2,\n",
        "        random_state=42,\n",
        "        n_estimators=11)),\n",
        "])\n",
        "\n",
        "pipe_forest.fit(X_train_scaled, Y_train)\n",
        "p1 = pipe_forest.predict(X_val_scaled)\n",
        "f1_forest = f1_score(Y_val, p1, average='macro')\n",
        "print(f\"F1_forest: {f1_forest}\")\n",
        "print(classification_report(Y_val, p1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904vCZsX2_ka"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IUDbgSH3Bxb",
        "outputId": "54193d3e-ca08-4a79-8185-afa4e81146e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25323266 -0.21132326]\n",
            "F1_mlp: 0.4723531846946242\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.36      0.40      4075\n",
            "           1       0.50      0.60      0.55      4464\n",
            "\n",
            "    accuracy                           0.48      8539\n",
            "   macro avg       0.48      0.48      0.47      8539\n",
            "weighted avg       0.48      0.48      0.48      8539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "pipe_mlp = Pipeline([\n",
        "    (\"surface\", SurfaceFeatureTransformer(feature_names)),\n",
        "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"clf\", MLPClassifier(\n",
        "        hidden_layer_sizes=(10, 5),\n",
        "        activation='tanh',\n",
        "        solver='adam',\n",
        "        max_iter=100,\n",
        "        alpha = 0.0001\n",
        "    )),])\n",
        "\n",
        "pipe_mlp.fit(X_train_scaled, Y_train)\n",
        "p1 = pipe_mlp.predict(X_val_scaled)\n",
        "f1_mlp = f1_score(Y_val, p1, average='macro')\n",
        "print(f\"F1_mlp: {f1_mlp}\")\n",
        "print(classification_report(Y_val, p1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_zvbgSN4exD"
      },
      "source": [
        "# LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX7zMISO3x_r",
        "outputId": "55636917-9864-4843-fb8b-f5a641afe32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25323266 -0.21132326]\n",
            "F1_svc: 0.533585841673839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.65      0.57      4075\n",
            "           1       0.58      0.43      0.49      4464\n",
            "\n",
            "    accuracy                           0.54      8539\n",
            "   macro avg       0.54      0.54      0.53      8539\n",
            "weighted avg       0.55      0.54      0.53      8539\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "pipe_svc = Pipeline([\n",
        "    (\"surface\", SurfaceFeatureTransformer(feature_names)),\n",
        "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"clf\", LinearSVC()),\n",
        "])\n",
        "pipe_svc.fit(X_train_scaled, Y_train)\n",
        "p1 = pipe_svc.predict(X_val_scaled)\n",
        "f1_svc = f1_score(Y_val, p1, average='macro')\n",
        "print(f\"F1_svc: {f1_svc}\")\n",
        "print(classification_report(Y_val, p1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SfqB2N9EbWG"
      },
      "source": [
        "# Combined vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hj_LZWe2BK51"
      },
      "outputs": [],
      "source": [
        "y_logreg = pipe_small.predict(X_val_scaled)\n",
        "y_tree = pipe_tree.predict(X_val_scaled)\n",
        "y_forest = pipe_forest.predict(X_val_scaled)\n",
        "y_mlp = pipe_mlp.predict(X_val_scaled)\n",
        "y_svc = pipe_svc.predict(X_val_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aIDUDDKBBbhj"
      },
      "outputs": [],
      "source": [
        "x_vect = []\n",
        "for i in range(len(y_logreg)):\n",
        "  x_vect.append([y_logreg[i], y_tree[i], y_svc[i]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "JUiYc5wkEDJ6",
        "outputId": "ad50fd15-e8d7-4ae5-fa57-672829fb819e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
            "  warnings.warn(\n",
            "d:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "}\n",
              "\n",
              "#sk-container-id-1.light {\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: black;\n",
              "  --sklearn-color-background: white;\n",
              "  --sklearn-color-border-box: black;\n",
              "  --sklearn-color-icon: #696969;\n",
              "}\n",
              "\n",
              "#sk-container-id-1.dark {\n",
              "  --sklearn-color-text-on-default-background: white;\n",
              "  --sklearn-color-background: #111;\n",
              "  --sklearn-color-border-box: white;\n",
              "  --sklearn-color-icon: #878787;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: center;\n",
              "  justify-content: center;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  display: none;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  overflow: visible;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-0);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-0);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".estimator-table {\n",
              "    font-family: monospace;\n",
              "}\n",
              "\n",
              ".estimator-table summary {\n",
              "    padding: .5rem;\n",
              "    cursor: pointer;\n",
              "}\n",
              "\n",
              ".estimator-table summary::marker {\n",
              "    font-size: 0.7rem;\n",
              "}\n",
              "\n",
              ".estimator-table details[open] {\n",
              "    padding-left: 0.1rem;\n",
              "    padding-right: 0.1rem;\n",
              "    padding-bottom: 0.3rem;\n",
              "}\n",
              "\n",
              ".estimator-table .parameters-table {\n",
              "    margin-left: auto !important;\n",
              "    margin-right: auto !important;\n",
              "    margin-top: 0;\n",
              "}\n",
              "\n",
              ".estimator-table .parameters-table tr:nth-child(odd) {\n",
              "    background-color: #fff;\n",
              "}\n",
              "\n",
              ".estimator-table .parameters-table tr:nth-child(even) {\n",
              "    background-color: #f6f6f6;\n",
              "}\n",
              "\n",
              ".estimator-table .parameters-table tr:hover {\n",
              "    background-color: #e0e0e0;\n",
              "}\n",
              "\n",
              ".estimator-table table td {\n",
              "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
              "}\n",
              "\n",
              "/*\n",
              "    `table td`is set in notebook with right text-align.\n",
              "    We need to overwrite it.\n",
              "*/\n",
              ".estimator-table table td.param {\n",
              "    text-align: left;\n",
              "    position: relative;\n",
              "    padding: 0;\n",
              "}\n",
              "\n",
              ".user-set td {\n",
              "    color:rgb(255, 94, 0);\n",
              "    text-align: left !important;\n",
              "}\n",
              "\n",
              ".user-set td.value {\n",
              "    color:rgb(255, 94, 0);\n",
              "    background-color: transparent;\n",
              "}\n",
              "\n",
              ".default td {\n",
              "    color: black;\n",
              "    text-align: left !important;\n",
              "}\n",
              "\n",
              ".user-set td i,\n",
              ".default td i {\n",
              "    color: black;\n",
              "}\n",
              "\n",
              "/*\n",
              "    Styles for parameter documentation links\n",
              "    We need styling for visited so jupyter doesn't overwrite it\n",
              "*/\n",
              "a.param-doc-link,\n",
              "a.param-doc-link:link,\n",
              "a.param-doc-link:visited {\n",
              "    text-decoration: underline dashed;\n",
              "    text-underline-offset: .3em;\n",
              "    color: inherit;\n",
              "    display: block;\n",
              "    padding: .5em;\n",
              "}\n",
              "\n",
              "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
              "a.param-doc-link::before {\n",
              "    position: absolute;\n",
              "    content: \"\";\n",
              "    inset: 0;\n",
              "}\n",
              "\n",
              ".param-doc-description {\n",
              "    display: none;\n",
              "    position: absolute;\n",
              "    z-index: 9999;\n",
              "    left: 0;\n",
              "    padding: .5ex;\n",
              "    margin-left: 1.5em;\n",
              "    color: var(--sklearn-color-text);\n",
              "    box-shadow: .3em .3em .4em #999;\n",
              "    width: max-content;\n",
              "    text-align: left;\n",
              "    max-height: 10em;\n",
              "    overflow-y: auto;\n",
              "\n",
              "    /* unfitted */\n",
              "    background: var(--sklearn-color-unfitted-level-0);\n",
              "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              "/* Fitted state for parameter tooltips */\n",
              ".fitted .param-doc-description {\n",
              "    /* fitted */\n",
              "    background: var(--sklearn-color-fitted-level-0);\n",
              "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".param-doc-link:hover .param-doc-description {\n",
              "    display: block;\n",
              "}\n",
              "\n",
              ".copy-paste-icon {\n",
              "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
              "    background-repeat: no-repeat;\n",
              "    background-size: 14px 14px;\n",
              "    background-position: 0;\n",
              "    display: inline-block;\n",
              "    width: 14px;\n",
              "    height: 14px;\n",
              "    cursor: pointer;\n",
              "}\n",
              "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
              "        <div class=\"estimator-table\">\n",
              "            <details>\n",
              "                <summary>Parameters</summary>\n",
              "                <table class=\"parameters-table\">\n",
              "                  <tbody>\n",
              "                    \n",
              "        <tr class=\"user-set\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('penalty',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=penalty,-%7B%27l1%27%2C%20%27l2%27%2C%20%27elasticnet%27%2C%20None%7D%2C%20default%3D%27l2%27\">\n",
              "            penalty\n",
              "            <span class=\"param-doc-description\">penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'<br><br>Specify the norm of the penalty:<br><br>- `None`: no penalty is added;<br>- `'l2'`: add a L2 penalty term and it is the default choice;<br>- `'l1'`: add a L1 penalty term;<br>- `'elasticnet'`: both L1 and L2 penalty terms are added.<br><br>.. warning::<br>   Some penalties may not work with some solvers. See the parameter<br>   `solver` below, to know the compatibility between the penalty and<br>   solver.<br><br>.. versionadded:: 0.19<br>   l1 penalty with SAGA solver (allowing 'multinomial' + L1)<br><br>.. deprecated:: 1.8<br>   `penalty` was deprecated in version 1.8 and will be removed in 1.10.<br>   Use `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for<br>   `penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for<br>   `'penalty='elasticnet'`.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">&#x27;l1&#x27;</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('C',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=C,-float%2C%20default%3D1.0\">\n",
              "            C\n",
              "            <span class=\"param-doc-description\">C: float, default=1.0<br><br>Inverse of regularization strength; must be a positive float.<br>Like in support vector machines, smaller values specify stronger<br>regularization. `C=np.inf` results in unpenalized logistic regression.<br>For a visual example on the effect of tuning the `C` parameter<br>with an L1 penalty, see:<br>:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">1.0</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('l1_ratio',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=l1_ratio,-float%2C%20default%3D0.0\">\n",
              "            l1_ratio\n",
              "            <span class=\"param-doc-description\">l1_ratio: float, default=0.0<br><br>The Elastic-Net mixing parameter, with `0 <= l1_ratio <= 1`. Setting<br>`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.<br>Any value between 0 and 1 gives an Elastic-Net penalty of the form<br>`l1_ratio * L1 + (1 - l1_ratio) * L2`.<br><br>.. warning::<br>   Certain values of `l1_ratio`, i.e. some penalties, may not work with some<br>   solvers. See the parameter `solver` below, to know the compatibility between<br>   the penalty and solver.<br><br>.. versionchanged:: 1.8<br>    Default value changed from None to 0.0.<br><br>.. deprecated:: 1.8<br>    `None` is deprecated and will be removed in version 1.10. Always use<br>    `l1_ratio` to specify the penalty type.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">0.0</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('dual',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=dual,-bool%2C%20default%3DFalse\">\n",
              "            dual\n",
              "            <span class=\"param-doc-description\">dual: bool, default=False<br><br>Dual (constrained) or primal (regularized, see also<br>:ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation<br>is only implemented for l2 penalty with liblinear solver. Prefer `dual=False`<br>when n_samples > n_features.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">False</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('tol',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=tol,-float%2C%20default%3D1e-4\">\n",
              "            tol\n",
              "            <span class=\"param-doc-description\">tol: float, default=1e-4<br><br>Tolerance for stopping criteria.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">0.0001</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('fit_intercept',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=fit_intercept,-bool%2C%20default%3DTrue\">\n",
              "            fit_intercept\n",
              "            <span class=\"param-doc-description\">fit_intercept: bool, default=True<br><br>Specifies if a constant (a.k.a. bias or intercept) should be<br>added to the decision function.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">True</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('intercept_scaling',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=intercept_scaling,-float%2C%20default%3D1\">\n",
              "            intercept_scaling\n",
              "            <span class=\"param-doc-description\">intercept_scaling: float, default=1<br><br>Useful only when the solver `liblinear` is used<br>and `self.fit_intercept` is set to `True`. In this case, `x` becomes<br>`[x, self.intercept_scaling]`,<br>i.e. a \"synthetic\" feature with constant value equal to<br>`intercept_scaling` is appended to the instance vector.<br>The intercept becomes<br>``intercept_scaling * synthetic_feature_weight``.<br><br>.. note::<br>    The synthetic feature weight is subject to L1 or L2<br>    regularization as all other features.<br>    To lessen the effect of regularization on synthetic feature weight<br>    (and therefore on the intercept) `intercept_scaling` has to be increased.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">1</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('class_weight',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=class_weight,-dict%20or%20%27balanced%27%2C%20default%3DNone\">\n",
              "            class_weight\n",
              "            <span class=\"param-doc-description\">class_weight: dict or 'balanced', default=None<br><br>Weights associated with classes in the form ``{class_label: weight}``.<br>If not given, all classes are supposed to have weight one.<br><br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.<br><br>Note that these weights will be multiplied with sample_weight (passed<br>through the fit method) if sample_weight is specified.<br><br>.. versionadded:: 0.17<br>   *class_weight='balanced'*</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">None</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('random_state',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=random_state,-int%2C%20RandomState%20instance%2C%20default%3DNone\">\n",
              "            random_state\n",
              "            <span class=\"param-doc-description\">random_state: int, RandomState instance, default=None<br><br>Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the<br>data. See :term:`Glossary <random_state>` for details.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">None</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"user-set\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('solver',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=solver,-%7B%27lbfgs%27%2C%20%27liblinear%27%2C%20%27newton-cg%27%2C%20%27newton-cholesky%27%2C%20%27sag%27%2C%20%27saga%27%7D%2C%20%20%20%20%20%20%20%20%20%20%20%20%20default%3D%27lbfgs%27\">\n",
              "            solver\n",
              "            <span class=\"param-doc-description\">solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'<br><br>Algorithm to use in the optimization problem. Default is 'lbfgs'.<br>To choose a solver, you might want to consider the following aspects:<br><br>- 'lbfgs' is a good default solver because it works reasonably well for a wide<br>  class of problems.<br>- For :term:`multiclass` problems (`n_classes >= 3`), all solvers except<br>  'liblinear' minimize the full multinomial loss, 'liblinear' will raise an<br>  error.<br>- 'newton-cholesky' is a good choice for<br>  `n_samples` >> `n_features * n_classes`, especially with one-hot encoded<br>  categorical features with rare categories. Be aware that the memory usage<br>  of this solver has a quadratic dependency on `n_features * n_classes`<br>  because it explicitly computes the full Hessian matrix.<br>- For small datasets, 'liblinear' is a good choice, whereas 'sag'<br>  and 'saga' are faster for large ones;<br>- 'liblinear' can only handle binary classification by default. To apply a<br>  one-versus-rest scheme for the multiclass setting one can wrap it with the<br>  :class:`~sklearn.multiclass.OneVsRestClassifier`.<br><br>.. warning::<br>   The choice of the algorithm depends on the penalty chosen (`l1_ratio=0`<br>   for L2-penalty, `l1_ratio=1` for L1-penalty and `0 < l1_ratio < 1` for<br>   Elastic-Net) and on (multinomial) multiclass support:<br><br>   ================= ======================== ======================<br>   solver            l1_ratio                 multinomial multiclass<br>   ================= ======================== ======================<br>   'lbfgs'           l1_ratio=0               yes<br>   'liblinear'       l1_ratio=1 or l1_ratio=0 no<br>   'newton-cg'       l1_ratio=0               yes<br>   'newton-cholesky' l1_ratio=0               yes<br>   'sag'             l1_ratio=0               yes<br>   'saga'            0<=l1_ratio<=1           yes<br>   ================= ======================== ======================<br><br>.. note::<br>   'sag' and 'saga' fast convergence is only guaranteed on features<br>   with approximately the same scale. You can preprocess the data with<br>   a scaler from :mod:`sklearn.preprocessing`.<br><br>.. seealso::<br>   Refer to the :ref:`User Guide <Logistic_regression>` for more<br>   information regarding :class:`LogisticRegression` and more specifically the<br>   :ref:`Table <logistic_regression_solvers>`<br>   summarizing solver/penalty supports.<br><br>.. versionadded:: 0.17<br>   Stochastic Average Gradient (SAG) descent solver. Multinomial support in<br>   version 0.18.<br>.. versionadded:: 0.19<br>   SAGA solver.<br>.. versionchanged:: 0.22<br>   The default solver changed from 'liblinear' to 'lbfgs' in 0.22.<br>.. versionadded:: 1.2<br>   newton-cholesky solver. Multinomial support in version 1.6.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">&#x27;liblinear&#x27;</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('max_iter',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=max_iter,-int%2C%20default%3D100\">\n",
              "            max_iter\n",
              "            <span class=\"param-doc-description\">max_iter: int, default=100<br><br>Maximum number of iterations taken for the solvers to converge.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">100</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('verbose',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=verbose,-int%2C%20default%3D0\">\n",
              "            verbose\n",
              "            <span class=\"param-doc-description\">verbose: int, default=0<br><br>For the liblinear and lbfgs solvers set verbose to any positive<br>number for verbosity.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">0</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('warm_start',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=warm_start,-bool%2C%20default%3DFalse\">\n",
              "            warm_start\n",
              "            <span class=\"param-doc-description\">warm_start: bool, default=False<br><br>When set to True, reuse the solution of the previous call to fit as<br>initialization, otherwise, just erase the previous solution.<br>Useless for liblinear solver. See :term:`the Glossary <warm_start>`.<br><br>.. versionadded:: 0.17<br>   *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">False</td>\n",
              "        </tr>\n",
              "    \n",
              "\n",
              "        <tr class=\"default\">\n",
              "            <td><i class=\"copy-paste-icon\"\n",
              "                 onclick=\"copyToClipboard('n_jobs',\n",
              "                          this.parentElement.nextElementSibling)\"\n",
              "            ></i></td>\n",
              "            <td class=\"param\">\n",
              "        <a class=\"param-doc-link\"\n",
              "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
              "            n_jobs\n",
              "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Does not have any effect.<br><br>.. deprecated:: 1.8<br>   `n_jobs` is deprecated in version 1.8 and will be removed in 1.10.</span>\n",
              "        </a>\n",
              "    </td>\n",
              "            <td class=\"value\">None</td>\n",
              "        </tr>\n",
              "    \n",
              "                  </tbody>\n",
              "                </table>\n",
              "            </details>\n",
              "        </div>\n",
              "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
              "    // Get the parameter prefix from the closest toggleable content\n",
              "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
              "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
              "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
              "\n",
              "    const originalStyle = element.style;\n",
              "    const computedStyle = window.getComputedStyle(element);\n",
              "    const originalWidth = computedStyle.width;\n",
              "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
              "\n",
              "    navigator.clipboard.writeText(fullParamName)\n",
              "        .then(() => {\n",
              "            element.style.width = originalWidth;\n",
              "            element.style.color = 'green';\n",
              "            element.innerHTML = \"Copied!\";\n",
              "\n",
              "            setTimeout(() => {\n",
              "                element.innerHTML = originalHTML;\n",
              "                element.style = originalStyle;\n",
              "            }, 2000);\n",
              "        })\n",
              "        .catch(err => {\n",
              "            console.error('Failed to copy:', err);\n",
              "            element.style.color = 'red';\n",
              "            element.innerHTML = \"Failed!\";\n",
              "            setTimeout(() => {\n",
              "                element.innerHTML = originalHTML;\n",
              "                element.style = originalStyle;\n",
              "            }, 2000);\n",
              "        });\n",
              "    return false;\n",
              "}\n",
              "\n",
              "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
              "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
              "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
              "    const paramName = element.parentElement.nextElementSibling\n",
              "        .textContent.trim().split(' ')[0];\n",
              "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
              "\n",
              "    element.setAttribute('title', fullParamName);\n",
              "});\n",
              "\n",
              "\n",
              "/**\n",
              " * Adapted from Skrub\n",
              " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
              " * @returns \"light\" or \"dark\"\n",
              " */\n",
              "function detectTheme(element) {\n",
              "    const body = document.querySelector('body');\n",
              "\n",
              "    // Check VSCode theme\n",
              "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
              "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
              "\n",
              "    if (themeKindAttr && themeNameAttr) {\n",
              "        const themeKind = themeKindAttr.toLowerCase();\n",
              "        const themeName = themeNameAttr.toLowerCase();\n",
              "\n",
              "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
              "            return \"dark\";\n",
              "        }\n",
              "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
              "            return \"light\";\n",
              "        }\n",
              "    }\n",
              "\n",
              "    // Check Jupyter theme\n",
              "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
              "        return 'dark';\n",
              "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
              "        return 'light';\n",
              "    }\n",
              "\n",
              "    // Guess based on a parent element's color\n",
              "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
              "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
              "    if (match) {\n",
              "        const [r, g, b] = [\n",
              "            parseFloat(match[1]),\n",
              "            parseFloat(match[2]),\n",
              "            parseFloat(match[3])\n",
              "        ];\n",
              "\n",
              "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
              "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
              "\n",
              "        if (luma > 180) {\n",
              "            // If the text is very bright we have a dark theme\n",
              "            return 'dark';\n",
              "        }\n",
              "        if (luma < 75) {\n",
              "            // If the text is very dark we have a light theme\n",
              "            return 'light';\n",
              "        }\n",
              "        // Otherwise fall back to the next heuristic.\n",
              "    }\n",
              "\n",
              "    // Fallback to system preference\n",
              "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
              "}\n",
              "\n",
              "\n",
              "function forceTheme(elementId) {\n",
              "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
              "    if (estimatorElement === null) {\n",
              "        console.error(`Element with id ${elementId} not found.`);\n",
              "    } else {\n",
              "        const theme = detectTheme(estimatorElement);\n",
              "        estimatorElement.classList.add(theme);\n",
              "    }\n",
              "}\n",
              "\n",
              "forceTheme('sk-container-id-1');</script></body>"
            ],
            "text/plain": [
              "LogisticRegression(penalty='l1', solver='liblinear')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_comb = LogisticRegression(solver = \"liblinear\", penalty=\"l1\")\n",
        "p_comb.fit(x_vect, Y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "r-rUa-JRqmRx"
      },
      "outputs": [],
      "source": [
        "df_test_add = pd.read_csv(\"data\\\\semeval\\\\additional\\\\add_data_clear.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(165833, 5)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_add.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9B3JAc2-1SYr"
      },
      "outputs": [],
      "source": [
        "Y_test_list_add = df_test_add.head(165833)[\"label\"].astype(int).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mH8fv8xJ3v1n"
      },
      "outputs": [],
      "source": [
        "data_path_feats_cross = \"data\\\\semeval\\\\additional\\\\test_cross_feats.csv\"\n",
        "df_cross = pd.read_csv(data_path_feats_cross)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yv_i6tbg3-qh"
      },
      "outputs": [],
      "source": [
        "X_test_cross = []\n",
        "for index, row in df_cross.head(165833).iterrows():\n",
        "  X_test_cross.append(list(row.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0eOTvODBoIMH"
      },
      "outputs": [],
      "source": [
        "X_test_add_comments = []\n",
        "for index, row in df_test_add.head(165833).iterrows():\n",
        "  code = row['code']\n",
        "  X_test_add_comments.append(get_comment_ratio(code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2C-UuW4U1Vxy"
      },
      "outputs": [],
      "source": [
        "X_test_add = []\n",
        "i = 0\n",
        "for index, row in df_test_add.head(165833).iterrows():\n",
        "  code = row['code']\n",
        "  loc = len(code.splitlines())\n",
        "  bucket = \"small\"\n",
        "  if loc >= 20 and loc <= 70:\n",
        "    bucket = \"medium\"\n",
        "  elif loc > 70:\n",
        "    bucket = \"large\"\n",
        "\n",
        "  # x_line_std = find_line_length_std(code)\n",
        "  # x_line_entropy = X_test_add_entropy[i]\n",
        "\n",
        "  # X_test_add.append([bucket, x_line_std, x_line_entropy] + X_test_cross[i][1:3] + X_test_cross[i][4:5] + X_test[i][6:]) #\n",
        "  comm_h = X_test_add_comments[i]\n",
        "  X_test_add.append([bucket, comm_h] + X_test_cross[i][1:2])\n",
        "  i = i + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8y48VgMO1mxI"
      },
      "outputs": [],
      "source": [
        "X_test_add = np.asarray(X_test_add, dtype=object)\n",
        "Y_test_add = np.asarray(Y_test_list_add)\n",
        "\n",
        "X_test_add_scaled = np.zeros((X_test_add.shape[0], X_test_add.shape[1] - 1), dtype=np.float32)\n",
        "\n",
        "for i, x in enumerate(X_test_add):\n",
        "    bucket = x[0]\n",
        "    feats = np.asarray(x[1:], dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    if bucket in bucket_scalers:\n",
        "        X_test_add_scaled[i] = bucket_scalers[bucket].transform(feats)[0]\n",
        "    else:\n",
        "        # fallback: no scaling (or use global_scaler if you have one)\n",
        "        X_test_add_scaled[i] = feats[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ou0lwo01sW-",
        "outputId": "97efb464-112f-4e14-89d7-13e3c07d1135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.6537140966550274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77    115995\n",
            "           1       0.49      0.58      0.53     49838\n",
            "\n",
            "    accuracy                           0.69    165833\n",
            "   macro avg       0.65      0.66      0.65    165833\n",
            "weighted avg       0.71      0.69      0.70    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p1_logreg = pipe_small.predict(X_test_add_scaled)\n",
        "f1_logreg = f1_score(Y_test_list_add, p1_logreg, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_test_list_add, p1_logreg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgEVIey54nhZ",
        "outputId": "b768541f-52b5-45f7-b73e-0e26037e4829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best threshold: 0.52\n"
          ]
        }
      ],
      "source": [
        "probs = clf.predict_proba(X_test_add_scaled)[:, 1]\n",
        "\n",
        "thresholds = np.linspace(0.1, 0.9, 81)\n",
        "scores = [\n",
        "    f1_score(Y_test_add, probs >= t, average=\"macro\")\n",
        "    for t in thresholds\n",
        "]\n",
        "\n",
        "best_t = thresholds[np.argmax(scores)]\n",
        "print(f\"Best threshold: {best_t}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2BCue8v4rr2",
        "outputId": "577d9e2c-b942-444e-d93d-1e90c695edcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.654828655928188\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.78    115995\n",
            "           1       0.50      0.57      0.53     49838\n",
            "\n",
            "    accuracy                           0.70    165833\n",
            "   macro avg       0.65      0.66      0.65    165833\n",
            "weighted avg       0.71      0.70      0.70    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = (clf.predict_proba(X_test_add_scaled)[:, 1] >= best_t)\n",
        "print(f1_score(Y_test_add, y_pred, average='macro'))\n",
        "print(classification_report(Y_test_add, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ErjKUq1pVpR",
        "outputId": "d58512eb-7071-4f22-ae0b-631c9dbdced7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.6390944272407749\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.79      0.78    115995\n",
            "           1       0.50      0.49      0.49     49838\n",
            "\n",
            "    accuracy                           0.70    165833\n",
            "   macro avg       0.64      0.64      0.64    165833\n",
            "weighted avg       0.70      0.70      0.70    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p1_tree = pipe_tree.predict(X_test_add_scaled)\n",
        "f1_logreg = f1_score(Y_test_list_add, p1_tree, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_test_list_add, p1_tree))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUwnC7v65Ltf",
        "outputId": "4b6b294e-aa20-4e59-95b9-edbbc0798db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.647341628926476\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.71      0.76    115995\n",
            "           1       0.48      0.61      0.54     49838\n",
            "\n",
            "    accuracy                           0.68    165833\n",
            "   macro avg       0.64      0.66      0.65    165833\n",
            "weighted avg       0.71      0.68      0.69    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p1_forest = pipe_forest.predict(X_test_add_scaled)\n",
        "f1_logreg = f1_score(Y_test_list_add, p1_forest, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_test_list_add, p1_forest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n25reW9l5QpR",
        "outputId": "b6085d3a-0b60-46e5-ad04-23cd494a3ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.6248932057227348\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.63      0.71    115995\n",
            "           1       0.44      0.68      0.54     49838\n",
            "\n",
            "    accuracy                           0.65    165833\n",
            "   macro avg       0.63      0.66      0.62    165833\n",
            "weighted avg       0.71      0.65      0.66    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p1_mlp = pipe_mlp.predict(X_test_add_scaled)\n",
        "f1_logreg = f1_score(Y_test_list_add, p1_mlp, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_test_list_add, p1_mlp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKToenx85Vrh",
        "outputId": "e830e04a-a53d-4982-c014-51eb79022abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1_logreg: 0.6548193374999066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.78    115995\n",
            "           1       0.50      0.58      0.53     49838\n",
            "\n",
            "    accuracy                           0.70    165833\n",
            "   macro avg       0.65      0.66      0.65    165833\n",
            "weighted avg       0.71      0.70      0.70    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p1_svc = pipe_svc.predict(X_test_add_scaled)\n",
        "f1_logreg = f1_score(Y_test_list_add, p1_svc, average='macro')\n",
        "print(f\"F1_logreg: {f1_logreg}\")\n",
        "print(classification_report(Y_test_list_add, p1_svc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8t9tutmGelc",
        "outputId": "03a29de5-107b-4575-a634-75afe4a59dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 macro0.6495244269623406\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.73      0.77    115995\n",
            "           1       0.49      0.59      0.53     49838\n",
            "\n",
            "    accuracy                           0.69    165833\n",
            "   macro avg       0.65      0.66      0.65    165833\n",
            "weighted avg       0.71      0.69      0.70    165833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_test_vect = []\n",
        "for i in range(len(p1_logreg)):\n",
        "  x_test_vect.append([p1_logreg[i], p1_tree[i], p1_svc[i]])\n",
        "\n",
        "y_pred_comb = p_comb.predict(x_test_vect)\n",
        "print(f\"f1 macro{f1_score(Y_test_list_add, y_pred_comb, average='macro')}\")\n",
        "print(classification_report(Y_test_list_add, y_pred_comb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3RLPJp6Azy",
        "outputId": "bf1a22ae-5656-469f-d0e8-dc83df20534d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4831\n"
          ]
        }
      ],
      "source": [
        "y_pred_combined = []\n",
        "i = 0\n",
        "for i in range(len(p1)):\n",
        "  c = p1_logreg[i] + p1_tree[i] + p1_svc[i]\n",
        "  if c >= 2:\n",
        "    y_pred_combined.append(1)\n",
        "  else:\n",
        "    y_pred_combined.append(0)\n",
        "print(len([y for y in y_pred_combined if y == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8539\n"
          ]
        }
      ],
      "source": [
        "print(len(y_pred_combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etqJ4xA58bNE",
        "outputId": "0b29c8cd-3c19-416e-c595-7a707b0fb41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77    115995\n",
            "           1       0.49      0.58      0.53     49838\n",
            "\n",
            "    accuracy                           0.70    165833\n",
            "   macro avg       0.65      0.66      0.65    165833\n",
            "weighted avg       0.71      0.70      0.70    165833\n",
            "\n",
            "f1 macro: 0.6535392698799782\n"
          ]
        }
      ],
      "source": [
        "# ensure predictions align with Y_test_list_add length\n",
        "n = len(Y_test_list_add)\n",
        "p1_logreg_aligned = np.asarray(p1_logreg)[:n]\n",
        "p1_tree_aligned = np.asarray(p1_tree)[:n]\n",
        "p1_svc_aligned = np.asarray(p1_svc)[:n]\n",
        "\n",
        "assert len(p1_logreg_aligned) == len(p1_tree_aligned) == len(p1_svc_aligned) == n\n",
        "\n",
        "y_pred_combined = [\n",
        "    1 if (a + b + c) >= 2 else 0\n",
        "    for a, b, c in zip(p1_logreg_aligned, p1_tree_aligned, p1_svc_aligned)\n",
        "]\n",
        "\n",
        "print(classification_report(Y_test_list_add, y_pred_combined))\n",
        "print(f\"f1 macro: {f1_score(Y_test_list_add, y_pred_combined, average='macro')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "u_95s65MW4Zn"
      },
      "outputs": [],
      "source": [
        "data_path_test_new2 ='data/semeval/test_new.parquet'\n",
        "df_test_new = pd.read_parquet(data_path_test_new2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "P4iqkMLR8QGm"
      },
      "outputs": [],
      "source": [
        "X_test_submit = []\n",
        "i = 0\n",
        "for index, row in df_test_new.head(500000).iterrows():\n",
        "  code = row['code']\n",
        "  bucket = \"small\"\n",
        "  if len(code.splitlines()) >= 20 and len(code.splitlines()) <= 70:\n",
        "    bucket = \"medium\"\n",
        "  elif len(code.splitlines()) > 70:\n",
        "    bucket = \"large\"\n",
        "\n",
        "  # x_line_entropy = find_line_len_entropy(code)\n",
        "\n",
        "\n",
        "  # X_test_submit.append([buc ket, x_line_std, x_line_entropy] + x_test[1:3] + x_test[4:5] + x_test[6:]) #\n",
        "  comm_h = get_comment_ratio(code)\n",
        "  X_test_submit.append([bucket, comm_h] + X_test[i][1:2])\n",
        "  i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": true,
        "id": "kZMJDno4B4D4"
      },
      "outputs": [],
      "source": [
        "X_test_submit_np = np.asarray(X_test_submit, dtype=object)\n",
        "\n",
        "X_test_submit_scaled = np.zeros((X_test_submit_np.shape[0], X_test_submit_np.shape[1] - 1), dtype=np.float32)\n",
        "\n",
        "for i, x in enumerate(X_test_submit):\n",
        "    bucket = x[0]\n",
        "    feats = np.asarray(x[1:], dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    if bucket in bucket_scalers:\n",
        "        X_test_submit_scaled[i] = bucket_scalers[bucket].transform(feats)[0]\n",
        "    else:\n",
        "        # fallback: no scaling (or use global_scaler if you have one)\n",
        "        X_test_submit_scaled[i] = feats[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uScUziI5trj",
        "outputId": "db143873-cac5-4f78-de62-dafe933ad473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07147478251379817\n",
            "0.17150626452977064\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "y_text_pred2 = []\n",
        "cnt0 = 0\n",
        "cnt1 = 0\n",
        "i = 0\n",
        "sum0 = 0\n",
        "sum1 = 0\n",
        "max0 = 0\n",
        "for x in X_train:\n",
        "  if x[2] > 0:\n",
        "    if Y_train[i] == 1:\n",
        "      cnt1 += 1\n",
        "      sum1 += x[2]\n",
        "    elif Y_train[i] == 0:\n",
        "      cnt0 += 1\n",
        "      sum0 += x[2]\n",
        "      if x[2] > max0:\n",
        "        max0 = x[2]\n",
        "  i = i + 1\n",
        "print(sum0/cnt0)\n",
        "\n",
        "print(sum1/cnt1)\n",
        "print(max0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "HEjKIVdzDA1A"
      },
      "outputs": [],
      "source": [
        "y_pred_submit = pipe_small.predict(X_test_submit_scaled)\n",
        "y_pred = (clf.predict_proba(X_test_submit_scaled)[:, 1] >= 0.65)\n",
        "y_pred_tree = pipe_tree.predict(X_test_submit_scaled)\n",
        "y_pred_forest = pipe_forest.predict(X_test_submit_scaled)\n",
        "y_pred_mlp = pipe_mlp.predict(X_test_submit_scaled)\n",
        "y_pred_svc = pipe_svc.predict(X_test_submit_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "OpuKta_x-ajD"
      },
      "outputs": [],
      "source": [
        "y_combined = []\n",
        "i = 0\n",
        "for i in range(len(y_pred_submit)):\n",
        "  c = y_pred_submit[i] + y_pred_tree[i] + y_pred_svc[i]\n",
        "  if c >= 2:\n",
        "    y_combined.append(1)\n",
        "  else:\n",
        "    y_combined.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKue-_T5-vAF",
        "outputId": "76bccd4d-0df6-444f-bd61-8d0d3ea612c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "351871\n"
          ]
        }
      ],
      "source": [
        "print(len([y for y in y_pred_svc if y == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4maFJlw-1Q-",
        "outputId": "b8c4f845-732a-4ff0-e18b-39f3716fc494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ID  label\n",
            "0    0      0\n",
            "1    2      0\n",
            "2    5      0\n",
            "3    6      0\n",
            "4    7      0\n",
            "5    8      0\n",
            "6    9      0\n",
            "7   10      0\n",
            "8   11      0\n",
            "9   12      0\n",
            "10  13      1\n",
            "11  15      0\n",
            "12  16      1\n",
            "13  17      1\n",
            "14  18      0\n",
            "15  20      0\n",
            "16  21      0\n",
            "17  22      0\n",
            "18  23      0\n",
            "19  24      0\n"
          ]
        }
      ],
      "source": [
        "submission_ids = df_test_new.head(500000)['ID'].values\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': submission_ids,\n",
        "    'label': y_pred_svc,\n",
        "})\n",
        "\n",
        "print(submission_df.head(20))\n",
        "\n",
        "submission_df.to_csv('subm_svc_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rth47AnK7gb6",
        "outputId": "f3a211fc-8d8c-4ad1-8ddb-37a7c26ef240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3423\n"
          ]
        }
      ],
      "source": [
        "y_text_pred = []\n",
        "cnt = 0\n",
        "i = 0\n",
        "for x in X_test:\n",
        "  if x[2] > 0.3:\n",
        "    if y_pred_submit[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred.append(1)\n",
        "  else:\n",
        "    y_text_pred.append(y_pred_submit[i])\n",
        "  i = i + 1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3ARCd3urWjs",
        "outputId": "a97342b2-c761-4d49-9099-7531092b75d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "383494\n"
          ]
        }
      ],
      "source": [
        "Y_pred_tree = pipe_tree.predict(X_test_submit_scaled)\n",
        "print(len([y for y in Y_pred_tree if y == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqXNuu170t6X",
        "outputId": "b993fcc4-0bff-4d7c-eada-0a604907ac87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4383\n"
          ]
        }
      ],
      "source": [
        "y_text_pred_tree = []\n",
        "cnt = 0\n",
        "i = 0\n",
        "for x in X_test:\n",
        "  if x[2] > 0.3:\n",
        "    if Y_pred_tree[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred_tree.append(1)\n",
        "  else:\n",
        "    y_text_pred_tree.append(Y_pred_tree[i])\n",
        "  i = i + 1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAMJU7yqD2F1",
        "outputId": "14c2b0d5-7d4b-40a8-a6b5-aea188b501dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "388347\n",
            "348285\n",
            "383494\n"
          ]
        }
      ],
      "source": [
        "print(len([y for y in y_pred if y == 0]))\n",
        "print(len([y for y in y_pred_submit if y == 0]))\n",
        "print(len([y for y in Y_pred_tree if y == 0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvfaSKVzWRYB",
        "outputId": "3f4e1db1-57ef-4081-9c63-946dd2605af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3534\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "cnt = 0\n",
        "for index, row in df_test_new.head(500000).iterrows():\n",
        "  code = row[\"code\"]\n",
        "  line = code.splitlines()[0]\n",
        "  loc = len(code.splitlines())\n",
        "  if \"go\" == line or \"python\" == line or \"java\" == line or \"c++\" == line or \"javascript\" == line or \"c#\" == line or line == \"c\" or line == \"cpp\":\n",
        "    if y_text_pred[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred[i] = 1\n",
        "\n",
        "  # if \"Example usage\" in code:\n",
        "  #   if y_pred[i] == 0:\n",
        "  #     cnt += 1\n",
        "  #   y_pred[i] = 1\n",
        "  if \"code here\" in code:\n",
        "    if y_text_pred[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred[i] = 1\n",
        "  if \"without comments\" in code and loc > 1:\n",
        "    if y_text_pred[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred[i] = 1\n",
        "  # # # if \"Example\" in code:\n",
        "  # # #   if y_test[i] == 0:\n",
        "  # # #     cnt += 1\n",
        "  # #   y_test[i] = 1\n",
        "  # if \"please\" in code or \"Please\" in code and loc > 1:\n",
        "  #   if y_text_pred[i] == 0:\n",
        "  #     cnt += 1\n",
        "  #   y_text_pred[i] = 1\n",
        "  if \"Explanation\" in code or \"explanations\" in code and loc > 1:\n",
        "    if y_text_pred[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred[i] = 1\n",
        "  if \"aplogize\" in code:\n",
        "    if y_text_pred[i] == 0:\n",
        "      cnt += 1\n",
        "    y_text_pred[i] = 1\n",
        "\n",
        "  i = i + 1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WuUcbVeIqHL",
        "outputId": "069e7e47-25f7-4b69-eb25-71c011dfa4b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "341328\n"
          ]
        }
      ],
      "source": [
        "print(len([y for y in y_text_pred if y == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD1nb7WpDH00",
        "outputId": "329a8eb2-6f75-45b3-f8eb-1bacacd9e148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ID  label\n",
            "0    0      0\n",
            "1    2      0\n",
            "2    5      0\n",
            "3    6      0\n",
            "4    7      0\n",
            "5    8      0\n",
            "6    9      0\n",
            "7   10      0\n",
            "8   11      0\n",
            "9   12      0\n",
            "10  13      1\n",
            "11  15      0\n",
            "12  16      1\n",
            "13  17      1\n",
            "14  18      0\n",
            "15  20      0\n",
            "16  21      0\n",
            "17  22      0\n",
            "18  23      1\n",
            "19  24      0\n"
          ]
        }
      ],
      "source": [
        "submission_ids = df_test_new.head(500000)['ID'].values\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': submission_ids,\n",
        "    'label': y_text_pred,\n",
        "})\n",
        "\n",
        "print(submission_df.head(20))\n",
        "\n",
        "submission_df.to_csv('subm_logreg_2f_rule_03_script_065_th.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBTn_VTz1sup"
      },
      "source": [
        "## Only old feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbsuGmkiPKGK",
        "outputId": "5021f2ef-53b8-4818-8c38-173ef2483b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "  feature  coefficient  abs_coefficient\n",
            "0  comm_h     1.171326         1.171326\n",
            "1    verb     0.539016         0.539016\n"
          ]
        }
      ],
      "source": [
        "clf = pipe_small.named_steps[\"clf\"]\n",
        "coef = clf.coef_[0]   # binary classification\n",
        "\n",
        "print(len(coef))\n",
        "print(len(feature_names))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"coefficient\": coef,\n",
        "    \"abs_coefficient\": abs(coef)\n",
        "}).sort_values(\"abs_coefficient\", ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "xyclSzJsQAHu"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 7 features, but StandardScaler is expecting 2 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m feats = np.asarray(x[\u001b[32m1\u001b[39m:], dtype=np.float32).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m bucket_scalers:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     X_test_scaled[i] = \u001b[43mbucket_scalers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# fallback: no scaling (or use global_scaler if you have one)\u001b[39;00m\n\u001b[32m     13\u001b[39m     X_test_scaled[i] = feats[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1094\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1091\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1093\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2923\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2923\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\stuff\\SemEvalTask13-SubtaskA\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2788\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2789\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2790\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: X has 7 features, but StandardScaler is expecting 2 features as input."
          ]
        }
      ],
      "source": [
        "X_test = np.asarray(X_test, dtype=object)\n",
        "\n",
        "X_test_scaled = np.zeros((X_test.shape[0], X_test.shape[1] - 1), dtype=np.float32)\n",
        "\n",
        "for i, x in enumerate(X_test):\n",
        "    bucket = x[0]\n",
        "    feats = np.asarray(x[1:], dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    if bucket in bucket_scalers:\n",
        "        X_test_scaled[i] = bucket_scalers[bucket].transform(feats)[0]\n",
        "    else:\n",
        "        # fallback: no scaling (or use global_scaler if you have one)\n",
        "        X_test_scaled[i] = feats[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8w6vAVJT9l5",
        "outputId": "2fcba992-0766-4626-9ee0-79b78d83b86a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'01-ai/Yi-Coder-9B-Chat', 'google/codegemma-2b', 'human', 'bigcode/starcoder2-7b', 'bigcode/starcoder', 'meta-llama/Llama-3.2-3B', 'Qwen/Qwen2.5-Coder-32B-Instruct', 'codellama/CodeLlama-34b-Instruct-hf', 'Qwen/Qwen2.5-Coder-1.5B-Instruct', '01-ai/Yi-Coder-1.5B', 'meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.3-70B-Instruct', '01-ai/Yi-Coder-9B', '01-ai/Yi-Coder-1.5B-Chat', 'codellama/CodeLlama-70b-Instruct-hf', 'deepseek-ai/deepseek-coder-6.7b-instruct', 'microsoft/Phi-3-medium-4k-instruct', 'Qwen/Qwen2.5-Coder-1.5B', 'meta-llama/Llama-3.2-1B', 'bigcode/starcoder2-15b', 'Qwen/Qwen2.5-Coder-7B', 'microsoft/Phi-3.5-mini-instruct', 'deepseek-ai/deepseek-coder-1.3b-instruct', 'deepseek-ai/deepseek-coder-6.7b-base', 'ibm-granite/granite-8b-code-base-4k', 'Qwen/Qwen2.5-Coder-7B-Instruct', 'google/codegemma-7b', 'bigcode/starcoder2-3b', 'codellama/CodeLlama-7b-hf', 'microsoft/Phi-3-small-8k-instruct', 'microsoft/phi-2', 'deepseek-ai/deepseek-coder-1.3b-base', 'ibm-granite/granite-8b-code-instruct-4k', 'microsoft/Phi-3-mini-4k-instruct', 'meta-llama/Llama-3.1-8B-Instruct'}\n"
          ]
        }
      ],
      "source": [
        "print(set(df_train[\"generator\"].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpySPapXT5pK",
        "outputId": "b11a2655-ce0e-440b-e785-09918a3db75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----NEW CODE ------\n",
            "def solve(a, b, c):\n",
            "    # Check for first shop cheaper\n",
            "    first_shop_cheaper = -1\n",
            "    for x in range(1, 10**9 + 1):\n",
            "        if a * x < c * (x + b - 1) // b:\n",
            "            first_shop_cheaper = x\n",
            "            break\n",
            "\n",
            "    # Check for second shop cheaper\n",
            "    second_shop_cheaper = -1\n",
            "    for x in range(1, 10**9 + 1):\n",
            "        if c * (x + b - 1) // b < a * x:\n",
            "            second_shop_cheaper = x\n",
            "            break\n",
            "\n",
            "    return first_shop_cheaper, second_shop_cheaper\n",
            "\n",
            "def main():\n",
            "    import sys\n",
            "    input = sys.stdin.read()\n",
            "    data = input.split()\n",
            "    t = int(data[0])\n",
            "    index = 1\n",
            "    results = []\n",
            "    for _ in range(t):\n",
            "        a = int(data[index])\n",
            "        b = int(data[index + 1])\n",
            "        c = int(data[index + 2])\n",
            "        index += 3\n",
            "        results.append(solve(a, b, c))\n",
            "    \n",
            "    for result in results:\n",
            "        print(result[0], result[1])\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "-----NEW CODE ------\n",
            "import java.util.HashSet;\n",
            "import java.util.Set;\n",
            "\n",
            "public class Solution {\n",
            "    public int minImpossibleOR(int[] nums) {\n",
            "        Set<Integer> set = new HashSet<>();\n",
            "        for (int num : nums) {\n",
            "            if ((num & (num - 1)) == 0) {\n",
            "                set.add(num);\n",
            "            }\n",
            "        }\n",
            "        int ans = 1;\n",
            "        while (set.contains(ans)) {\n",
            "            ans <<= 1;\n",
            "        }\n",
            "        return ans;\n",
            "    }\n",
            "}\n",
            "-----NEW CODE ------\n",
            "import sys\n",
            "\n",
            "def find_dominant_piranha(a):\n",
            "    n = len(a)\n",
            "    for i in range(n):\n",
            "        if (i == 0 and a[i] > a[i+1]) or \\\n",
            "           (i == n-1 and a[i] > a[i-1]) or \\\n",
            "           (0 < i < n-1 and a[i] > a[i-1] and a[i] > a[i+1]):\n",
            "            return i + 1\n",
            "    return -1\n",
            "\n",
            "def main():\n",
            "    import sys\n",
            "    input = sys.stdin.read().split()\n",
            "    index = 0\n",
            "    t = int(input[index])\n",
            "    index += 1\n",
            "    results = []\n",
            "    for _ in range(t):\n",
            "        n = int(input[index])\n",
            "        index += 1\n",
            "        a = list(map(int, input[index:index + n]))\n",
            "        index += n\n",
            "        result = find_dominant_piranha(a)\n",
            "        results.append(result)\n",
            "    for result in results:\n",
            "        print(result)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "-----NEW CODE ------\n",
            "from collections import defaultdict\n",
            "\n",
            "def max_subset_with_pow2_distance(points):\n",
            "    points.sort()\n",
            "    n = len(points)\n",
            "    \n",
            "    best_subset = [points[0]]\n",
            "    best_size = 1\n",
            "    \n",
            "    for i in range(n):\n",
            "        current = points[i]\n",
            "        dp = defaultdict(int)\n",
            "        dp[current] = 1\n",
            "        \n",
            "        for j in range(i+1, n):\n",
            "            next_point = points[j]\n",
            "            diff = next_point - current\n",
            "            \n",
            "            if (diff & (diff - 1)) == 0:  # Check if diff is a power of 2\n",
            "                dp[next_point] = max(dp[current] + 1, dp[next_point])\n",
            "            \n",
            "                if dp[next_point] > best_size:\n",
            "                    best_size = dp[next_point]\n",
            "                    best_subset = [x for x in dp if dp[x] >= dp[next_point]]\n",
            "                elif dp[next_point] == best_size:\n",
            "                    candidate_subset = [x for x in dp if dp[x] >= dp[next_point]]\n",
            "                    if len(candidate_subset) > len(best_subset):\n",
            "                        best_subset = candidate_subset\n",
            "\n",
            "    return best_size, sorted(best_subset)\n",
            "\n",
            "n = int(input().strip())\n",
            "points = list(map(int, input().strip().split()))\n",
            "\n",
            "size, subset = max_subset_with_pow2_distance(points)\n",
            "print(size)\n",
            "print(' '.join(map(str, subset)))\n",
            "-----NEW CODE ------\n",
            "def main():\n",
            "    import sys\n",
            "    input = sys.stdin.read\n",
            "    data = input().split()\n",
            "    \n",
            "    p = [float(data[i]) for i in range(6)]\n",
            "    N = int(data[6])\n",
            "    \n",
            "    E_X = sum(i * p[i-1] for i in range(1, 7))\n",
            "    Var_X = sum((i - E_X) ** 2 * p[i-1] for i in range(1, 7))\n",
            "    \n",
            "    E_S = N * E_X\n",
            "    Var_S = (N-1) * Var_X + E_X\n",
            "    \n",
            "    print(f\"{E_S:.11f}\")\n",
            "    print(f\"{Var_S:.11f}\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "-----NEW CODE ------\n",
            "To solve this problem, we need to maximize the final element in the sequence by repeatedly performing the specified operations. Here’s a step-by-step approach to achieve this:\n",
            "\n",
            "1. **Identify the Optimal Strategy**:\n",
            "   - We need to maximize the final sum, which means we should prefer operations that combine larger elements early.\n",
            "   - By carefully selecting which element to replace or remove, we can influence the future sums and maximize the final element.\n",
            "\n",
            "2. **Dynamic Programming Approach**:\n",
            "   - Let's define `dp[i][j]` as the maximum final element we can get from the subsequence `a[i:j+1]`.\n",
            "   - The recurrence relation would be:\n",
            "     - `dp[i][j] = max(dp[i][k-1] + dp[k+1][j] + a[k-1] + a[k+1], dp[i][k-1] + dp[k+1][j] + a[k-2] + a[k] + a[k+1], ...)` where `k` is the chosen index to replace or remove.\n",
            "   - To build the dp table, we need to consider all possible sub-sequences and find the optimal way to reduce each sub-sequence to a single element.\n",
            "\n",
            "3. **Recording the Operations**:\n",
            "   - We need to keep track of which element is chosen in each operation to generate the required output format.\n",
            "   - This can be done using another array to store the indices of the chosen elements.\n",
            "\n",
            "4. **Implementation**:\n",
            "   - We will use a 2D list to store the maximum values for each subsequence.\n",
            "   - Another 2D list will store the indices of the chosen elements for each subsequence.\n",
            "   - Iterate through each possible subsequence length and calculate the maximum final element using the defined recurrence relation.\n",
            "   - After filling the DP table, backtrack through the stored operations to reconstruct the sequence of chosen indices.\n",
            "\n",
            "Here’s the Python code implementing this approach:\n",
            "\n",
            "```python\n",
            "def max_final_element(N, a):\n",
            "    import sys\n",
            "    sys.setrecursionlimit(10000)\n",
            "    \n",
            "    # Initialize dp and path arrays\n",
            "    dp = [[-float('inf')] * N for _ in range(N)]\n",
            "    path = [[-1] * N for _ in range(N)]\n",
            "    \n",
            "    # Base case for single elements\n",
            "    for i in range(N):\n",
            "        dp[i][i] = a[i]\n",
            "        path[i][i] = i\n",
            "    \n",
            "    # Fill the dp table\n",
            "    for\n",
            "-----NEW CODE ------\n",
            "To solve this problem, we can use a binary search approach combined with a graph traversal algorithm (like Dijkstra's algorithm) to find the minimum time Snuke is exposed to cosmic rays during his travel. Here's a step-by-step outline and the corresponding Python code:\n",
            "\n",
            "1. **Binary Search for Time**: We will perform a binary search on the time `T` Snuke is exposed to cosmic rays. The lower bound `low` is 0 (no exposure) and the upper bound `high` is the Euclidean distance between the starting and ending points (complete exposure).\n",
            "\n",
            "2. **Graph Representation**: For each time `T` in the binary search, we will construct a graph where nodes represent points outside the barrier polygons, and edges represent possible paths between these points. We will check if there is a path from the start to the end point using Dijkstra's algorithm within the given time `T`.\n",
            "\n",
            "3. **Feasibility Check**: For each candidate time `T` in the binary search, we will check if Snuke can travel from the start to the end point without being in the barriers for more than `T` time. We will treat the problem as involving Voronoi cells and potential barriers formed by the circles.\n",
            "\n",
            "4. **Dijkstra's Algorithm**: We will use Dijkstra's algorithm to find the shortest path in the constructed graph. We will keep track of the time spent outside the barriers and ensure it is minimized.\n",
            "\n",
            "Here's the Python code implementing the above approach:\n",
            "\n",
            "```python\n",
            "import sys\n",
            "import heapq\n",
            "import math\n",
            "from collections import deque\n",
            "\n",
            "def read_input():\n",
            "    import sys\n",
            "    import sys\n",
            "    import sys\n",
            "    import sys\n",
            "    import sys\n",
            "    import sys\n",
            "    import sys\n",
            "    input = sys.stdin.read().split()\n",
            "    xs, ys, xt, yt = map(int, input[:4])\n",
            "    N = int(input[4])\n",
            "    circles = [[int(input[4+i*3]), int(input[4+i*3+1]), int(input[4+i*3+2])] for i in range(N)]\n",
            "    return (xs, ys, xt, yt), N, circles\n",
            "\n",
            "def distance(p1, p2):\n",
            "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
            "\n",
            "def intersection_time(p1, p2, c, r):\n",
            "    x1, y1 = p1\n",
            "    x2,\n",
            "-----NEW CODE ------\n",
            "def is_prime(num):\n",
            "    if num <= 1:\n",
            "        return False\n",
            "    if num <= 3:\n",
            "        return True\n",
            "    if num % 2 == 0 or num % 3 == 0:\n",
            "        return False\n",
            "    i = 5\n",
            "    while i * i <= num:\n",
            "        if num % i == 0 or num % (i + 2) == 0:\n",
            "            return False\n",
            "        i += 6\n",
            "    return True\n",
            "\n",
            "def find_non_prime_m(n):\n",
            "    m = 2\n",
            "    while True:\n",
            "        if is_prime(m) and not is_prime(n + m):\n",
            "            return m\n",
            "        m += 1\n",
            "\n",
            "def main():\n",
            "    import sys\n",
            "    input = sys.stdin.read().split()\n",
            "    t = int(input[0])\n",
            "    results = []\n",
            "    index = 1\n",
            "    for _ in range(t):\n",
            "        n = int(input[index])\n",
            "        m = find_non_prime_m(n)\n",
            "        results.append(str(m))\n",
            "        index += 1\n",
            "    print(\"\\n\".join(results))\n",
            "\n",
            "main()\n",
            "-----NEW CODE ------\n",
            "def solve():\n",
            "    import sys\n",
            "    import heapq\n",
            "    input = sys.stdin.read().split()\n",
            "    index = 0\n",
            "    t = int(input[index])\n",
            "    index += 1\n",
            "    results = []\n",
            "    \n",
            "    for _ in range(t):\n",
            "        n = int(input[index])\n",
            "        k = int(input[index + 1])\n",
            "        index += 2\n",
            "        a = list(map(int, input[index:index + n]))\n",
            "        index += n\n",
            "        \n",
            "        max_heap = [-x for x in a]\n",
            "        heapq.heapify(max_heap)\n",
            "        \n",
            "        for _ in range(k):\n",
            "            max_val = -heapq.heappop(max_heap)\n",
            "            min_val = -max_heap[-1]\n",
            "            if max_val > min_val:\n",
            "                heapq.heappush(max_heap, -(max_val - 1))\n",
            "                heapq.heapreplace(max_heap, -(min_val + 1))\n",
            "            else:\n",
            "                break\n",
            "        \n",
            "        result = [-x for x in max_heap]\n",
            "        results.append(\" \".join(map(str, result)))\n",
            "    \n",
            "    print(\"\\n\".join(results))\n",
            "\n",
            "solve()\n",
            "-----NEW CODE ------\n",
            "def decentNumber(n):\n",
            "    # Initialize variables to store counts of 5's and 3's\n",
            "    count_of_5 = 0\n",
            "    count_of_3 = 0\n",
            "    \n",
            "    # Try to find the maximum number of 5's that satisfy the conditions\n",
            "    while n > 0:\n",
            "        if n % 3 == 0:\n",
            "            count_of_5 = n\n",
            "            break\n",
            "        n -= 5\n",
            "    \n",
            "    # Calculate the number of 3's\n",
            "    if n >= 0:\n",
            "        count_of_3 = 5 * (n // 5)\n",
            "        print('5' * count_of_5 + '3' * count_of_3)\n",
            "    else:\n",
            "        print(-1)\n",
            "\n",
            "def main():\n",
            "    import sys\n",
            "    input = sys.stdin.read\n",
            "    data = input().split()\n",
            "    \n",
            "    t = int(data[0])\n",
            "    index = 1\n",
            "    \n",
            "    for _ in range(t):\n",
            "        n = int(data[index])\n",
            "        decentNumber(n)\n",
            "        index += 1\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "This code defines a function `decentNumber` that calculates the required Decent Number based on the given length `n` and prints it. The main function reads the input and processes multiple test cases. This approach ensures that the largest possible Decent Number is found by maximally using the digit '5' first, then filling the remaining length with '3' if necessary.\n"
          ]
        }
      ],
      "source": [
        "j = 0\n",
        "for index, row in df_train.iterrows():\n",
        "  code = row[\"code\"]\n",
        "  loc = len(code.splitlines())\n",
        "  if loc <= 50 and row[\"generator\"] == \"Qwen/Qwen2.5-Coder-32B-Instruct\" and j < 10:\n",
        "    print(\"-----NEW CODE ------\")\n",
        "    print(code)\n",
        "    j = j + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYoHooI7Vq3j",
        "outputId": "fbee2d40-329c-4a39-f26a-5089e4114ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for index, row in df_test_new.iterrows():\n",
        "  if \"bigcode/starcoder2-7b\" in row[\"code\"]:\n",
        "    cnt += 1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvTzDpYXrWvt",
        "outputId": "690ee8f4-e83d-450e-e7a6-a74c463e5f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n",
            "----- NEW CODE ----\n",
            "python\n",
            "def check_TPrime(nums):\n",
            "    for num in nums:\n",
            "        divisors = [i for i in range(1, num+1) if num % i == 0]\n",
            "        if len(divisors) == 3:\n",
            "            print(\"YES\")\n",
            "        else:\n",
            "            print(\"NO\")\n",
            "\n",
            "n = int(input())\n",
            "nums = list(map(int, input().split()))\n",
            "check_TPrime(nums)\n"
          ]
        }
      ],
      "source": [
        "j = 0\n",
        "for index, row in df_train.iterrows():\n",
        "  loc = len(row[\"code\"].splitlines())\n",
        "  if loc < 10 and j < 10:\n",
        "    print(\"----- NEW CODE ----\")\n",
        "    print(code)\n",
        "    j = j + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Ablation & Multi-Model Comparison\n",
        "\n",
        "Grid-search across **Logistic Regression, XGBoost / Gradient Boosting, Random Forest, MLP, and a deeper neural network**, testing every feature present in the processed datasets (included vs excluded). Results are evaluated with **F1-macro on the validation set** (same metric used above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data files...\n",
            "  df_train: (500000, 4)\n",
            "  df_feats_train: (300000, 8)\n",
            "  df_test_add: (165833, 5)\n",
            "  df_cross: (165833, 8)\n",
            "\n",
            "Computing comment ratios for train (300k samples)...\n",
            "Computing comment ratios for test_add (165k samples)...\n",
            "\n",
            "✓ Data prepared successfully!\n",
            "  X_full_tr: (300000, 11)  |  X_full_te: (165833, 11)\n",
            "  y_tr: (300000,)  |  y_te: (165833,)\n",
            "  Features (11): ['verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total', 'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments', 'comments_text_like_ratio_comments', 'error_near_eof_ratio', 'comment_ratio', 'bucket_large', 'bucket_medium', 'bucket_small']\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# DATA PREPARATION FOR EXPERIMENTS\n",
        "# (Minimal setup - loads data and builds feature matrices)\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- 1. Load data files ---\n",
        "print(\"Loading data files...\")\n",
        "df_train = pd.read_parquet(\"data\\\\semeval\\\\train.parquet\")\n",
        "df_feats_train = pd.read_csv(\"data\\\\semeval\\\\processed\\\\train_feats_300k.csv\")\n",
        "df_test_add = pd.read_csv(\"data\\\\semeval\\\\additional\\\\add_data_clear.csv\")\n",
        "df_cross = pd.read_csv(\"data\\\\semeval\\\\additional\\\\test_cross_feats.csv\")\n",
        "\n",
        "print(f\"  df_train: {df_train.shape}\")\n",
        "print(f\"  df_feats_train: {df_feats_train.shape}\")\n",
        "print(f\"  df_test_add: {df_test_add.shape}\")\n",
        "print(f\"  df_cross: {df_cross.shape}\")\n",
        "\n",
        "# --- 2. Extract labels ---\n",
        "Y_train = df_train.head(300000)['label'].tolist()\n",
        "Y_test_list_add = df_test_add.head(165833)[\"label\"].astype(int).to_numpy()\n",
        "\n",
        "# --- 3. Compute comment ratios ---\n",
        "COMMENT_RE = re.compile(r\"(#|//|/\\*|\\*/)\")\n",
        "\n",
        "def get_comment_ratio(code: str) -> float:\n",
        "    lines = code.splitlines()\n",
        "    loc = len(lines)\n",
        "    comment_lines = 0\n",
        "    for line in lines:\n",
        "        if COMMENT_RE.search(line) and \"@\" not in line:\n",
        "            comment_lines += 1\n",
        "            if not line.strip().startswith((\"#\", \"//\")):\n",
        "                comment_lines -= 1\n",
        "    return comment_lines / loc if loc > 0 else 0\n",
        "\n",
        "print(\"\\nComputing comment ratios for train (300k samples)...\")\n",
        "X_comments_train = [get_comment_ratio(row['code']) for _, row in df_train.head(300000).iterrows()]\n",
        "\n",
        "print(\"Computing comment ratios for test_add (165k samples)...\")\n",
        "X_test_add_comments = [get_comment_ratio(row['code']) for _, row in df_test_add.head(165833).iterrows()]\n",
        "\n",
        "# --- 4. Build full feature matrices ---\n",
        "_N_TEST = 165833\n",
        "NUMERIC_CSV_COLS = list(df_feats_train.columns[1:])  # skip 'bucket' (str)\n",
        "df_cross_h = df_cross.head(_N_TEST)\n",
        "\n",
        "# Numeric features from CSV\n",
        "tr_numeric = np.nan_to_num(df_feats_train[NUMERIC_CSV_COLS].values.astype(np.float64))\n",
        "te_numeric = np.nan_to_num(df_cross_h[NUMERIC_CSV_COLS].values.astype(np.float64))\n",
        "\n",
        "# Comment ratio column\n",
        "tr_comment = np.asarray(X_comments_train, dtype=np.float64).reshape(-1, 1)\n",
        "te_comment = np.asarray(X_test_add_comments, dtype=np.float64).reshape(-1, 1)\n",
        "\n",
        "# Bucket → one-hot\n",
        "lb = LabelBinarizer()\n",
        "tr_bucket_oh = lb.fit_transform(df_feats_train[\"bucket\"].values)\n",
        "te_bucket_oh = lb.transform(df_cross_h[\"bucket\"].values)\n",
        "bucket_names = [f\"bucket_{c}\" for c in lb.classes_]\n",
        "\n",
        "# Stack: 7 numeric + 1 comment_ratio + 3 bucket dummies = 11 features\n",
        "X_full_tr = np.column_stack([tr_numeric, tr_comment, tr_bucket_oh])\n",
        "X_full_te = np.column_stack([te_numeric, te_comment, te_bucket_oh])\n",
        "\n",
        "ALL_FEAT = NUMERIC_CSV_COLS + [\"comment_ratio\"] + bucket_names\n",
        "y_tr = np.asarray(Y_train, dtype=int)\n",
        "y_te = np.asarray(Y_test_list_add, dtype=int)\n",
        "\n",
        "print(f\"\\n✓ Data prepared successfully!\")\n",
        "print(f\"  X_full_tr: {X_full_tr.shape}  |  X_full_te: {X_full_te.shape}\")\n",
        "print(f\"  y_tr: {y_tr.shape}  |  y_te: {y_te.shape}\")\n",
        "print(f\"  Features ({len(ALL_FEAT)}): {ALL_FEAT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] xgboost not installed — using GradientBoostingClassifier\n",
            "CSV columns: ['bucket', 'verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total', 'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments', 'comments_text_like_ratio_comments', 'error_near_eof_ratio']\n",
            "Train CSV: (300000, 8)  |  Test-Add CSV: (165833, 8)\n",
            "\n",
            "Numeric features from CSV (7):\n",
            "  verb_ratio_comments                        train [0.0000 .. 1.0000]  test  [0.0000 .. 1.0000]\n",
            "  text_like_ratio                            train [0.0000 .. 1.0000]  test  [0.0000 .. 1.0000]\n",
            "  comments_code_like_ratio_to_total          train [0.0000 .. 1.0000]  test  [0.0000 .. 1.4545]\n",
            "  comments_text_like_ratio_to_total          train [0.0000 .. 1.0000]  test  [0.0000 .. 0.8750]\n",
            "  comments_code_like_ratio_comments          train [0.0000 .. 1.0000]  test  [0.0000 .. 1.0000]\n",
            "  comments_text_like_ratio_comments          train [0.0000 .. 1.0000]  test  [0.0000 .. 1.0000]\n",
            "  error_near_eof_ratio                       train [0.0000 .. 1.0000]  test  [0.0000 .. 1.0000]\n",
            "\n",
            "Bucket distribution (train):\n",
            "bucket\n",
            "small     135173\n",
            "medium    130594\n",
            "large      34233\n",
            "\n",
            "Bucket distribution (test_add):\n",
            "bucket\n",
            "medium    73393\n",
            "small     51443\n",
            "large     40997\n",
            "\n",
            "Extra feature: comment_ratio (computed from raw code)\n",
            "  X_comments_train: len=300000, range [0.0000 .. 1.0000]\n",
            "\n",
            "Bucket one-hot classes: [np.str_('large'), np.str_('medium'), np.str_('small')] → 3 cols\n",
            "\n",
            "Full feature matrix — Train: (300000, 11)   Test-Add: (165833, 11)\n",
            "All features (11): ['verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total', 'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments', 'comments_text_like_ratio_comments', 'error_near_eof_ratio', 'comment_ratio', 'bucket_large', 'bucket_medium', 'bucket_small']\n",
            "\n",
            "21 feature subsets defined\n",
            "[  1/105] LogReg          | ALL                                           | CV=0.7618  Test=0.6003  (23s)\n",
            "[  2/105] LogReg          | drop_verb_ratio_comments                      | CV=0.7621  Test=0.5956  (37s)\n",
            "[  3/105] LogReg          | drop_text_like_ratio                          | CV=0.7487  Test=0.6002  (45s)\n",
            "[  4/105] LogReg          | drop_comments_code_like_ratio_to_total        | CV=0.7623  Test=0.5989  (53s)\n",
            "[  5/105] LogReg          | drop_comments_text_like_ratio_to_total        | CV=0.7584  Test=0.5869  (60s)\n",
            "[  6/105] LogReg          | drop_comments_code_like_ratio_comments        | CV=0.7618  Test=0.6003  (68s)\n",
            "[  7/105] LogReg          | drop_comments_text_like_ratio_comments        | CV=0.7618  Test=0.6003  (76s)\n",
            "[  8/105] LogReg          | drop_error_near_eof_ratio                     | CV=0.7341  Test=0.6209  (84s)\n",
            "[  9/105] LogReg          | drop_comment_ratio                            | CV=0.7577  Test=0.5952  (93s)\n",
            "[ 10/105] LogReg          | drop_bucket_large                             | CV=0.7618  Test=0.6002  (103s)\n",
            "[ 11/105] LogReg          | drop_bucket_medium                            | CV=0.7617  Test=0.6003  (112s)\n",
            "[ 12/105] LogReg          | drop_bucket_small                             | CV=0.7618  Test=0.6003  (122s)\n",
            "[ 13/105] LogReg          | only_verb_ratio_comments                      | CV=0.5877  Test=0.6658  (123s)\n",
            "[ 14/105] LogReg          | only_text_like_ratio                          | CV=0.6785  Test=0.5346  (124s)\n",
            "[ 15/105] LogReg          | only_comments_code_like_ratio_to_total        | CV=0.5725  Test=0.5969  (125s)\n",
            "[ 16/105] LogReg          | only_comments_text_like_ratio_to_total        | CV=0.6131  Test=0.6900  (127s)\n",
            "[ 17/105] LogReg          | only_comments_code_like_ratio_comments        | CV=0.5712  Test=0.5938  (128s)\n",
            "[ 18/105] LogReg          | only_comments_text_like_ratio_comments        | CV=0.5712  Test=0.5938  (129s)\n",
            "[ 19/105] LogReg          | only_error_near_eof_ratio                     | CV=0.5907  Test=0.4074  (130s)\n",
            "[ 20/105] LogReg          | only_comment_ratio                            | CV=0.6735  Test=0.6497  (131s)\n",
            "[ 21/105] LogReg          | prev_2feats                                   | CV=0.6803  Test=0.6464  (132s)\n",
            "[ 22/105] RandomForest    | ALL                                           | CV=0.7893  Test=0.5829  (296s)\n",
            "[ 23/105] RandomForest    | drop_verb_ratio_comments                      | CV=0.7890  Test=0.5826  (435s)\n",
            "[ 24/105] RandomForest    | drop_text_like_ratio                          | CV=0.7747  Test=0.5896  (589s)\n",
            "[ 25/105] RandomForest    | drop_comments_code_like_ratio_to_total        | CV=0.7880  Test=0.5824  (767s)\n",
            "[ 26/105] RandomForest    | drop_comments_text_like_ratio_to_total        | CV=0.7878  Test=0.5689  (937s)\n",
            "[ 27/105] RandomForest    | drop_comments_code_like_ratio_comments        | CV=0.7893  Test=0.5796  (1102s)\n",
            "[ 28/105] RandomForest    | drop_comments_text_like_ratio_comments        | CV=0.7893  Test=0.5796  (1268s)\n",
            "[ 29/105] RandomForest    | drop_error_near_eof_ratio                     | CV=0.7513  Test=0.6198  (1429s)\n",
            "[ 30/105] RandomForest    | drop_comment_ratio                            | CV=0.7800  Test=0.5887  (1579s)\n",
            "[ 31/105] RandomForest    | drop_bucket_large                             | CV=0.7892  Test=0.5877  (1751s)\n",
            "[ 32/105] RandomForest    | drop_bucket_medium                            | CV=0.7893  Test=0.5821  (1913s)\n",
            "[ 33/105] RandomForest    | drop_bucket_small                             | CV=0.7892  Test=0.5815  (2093s)\n",
            "[ 34/105] RandomForest    | only_verb_ratio_comments                      | CV=0.5878  Test=0.6657  (2155s)\n",
            "[ 35/105] RandomForest    | only_text_like_ratio                          | CV=0.6983  Test=0.5767  (2287s)\n",
            "[ 36/105] RandomForest    | only_comments_code_like_ratio_to_total        | CV=0.5731  Test=0.6025  (2362s)\n",
            "[ 37/105] RandomForest    | only_comments_text_like_ratio_to_total        | CV=0.6139  Test=0.6932  (2435s)\n",
            "[ 38/105] RandomForest    | only_comments_code_like_ratio_comments        | CV=0.5716  Test=0.5960  (2482s)\n",
            "[ 39/105] RandomForest    | only_comments_text_like_ratio_comments        | CV=0.5716  Test=0.5960  (2529s)\n",
            "[ 40/105] RandomForest    | only_error_near_eof_ratio                     | CV=0.5940  Test=0.4066  (2578s)\n",
            "[ 41/105] RandomForest    | only_comment_ratio                            | CV=0.6804  Test=0.6593  (2675s)\n",
            "[ 42/105] RandomForest    | prev_2feats                                   | CV=0.6922  Test=0.6452  (2769s)\n",
            "[ 43/105] GradBoost       | ALL                                           | CV=0.7886  Test=0.5809  (3103s)\n",
            "[ 44/105] GradBoost       | drop_verb_ratio_comments                      | CV=0.7887  Test=0.5829  (3402s)\n",
            "[ 45/105] GradBoost       | drop_text_like_ratio                          | CV=0.7745  Test=0.5901  (3683s)\n",
            "[ 46/105] GradBoost       | drop_comments_code_like_ratio_to_total        | CV=0.7878  Test=0.5837  (3953s)\n",
            "[ 47/105] GradBoost       | drop_comments_text_like_ratio_to_total        | CV=0.7875  Test=0.5813  (4256s)\n",
            "[ 48/105] GradBoost       | drop_comments_code_like_ratio_comments        | CV=0.7886  Test=0.5809  (4565s)\n",
            "[ 49/105] GradBoost       | drop_comments_text_like_ratio_comments        | CV=0.7886  Test=0.5809  (4877s)\n",
            "[ 50/105] GradBoost       | drop_error_near_eof_ratio                     | CV=0.7511  Test=0.6213  (5177s)\n",
            "[ 51/105] GradBoost       | drop_comment_ratio                            | CV=0.7796  Test=0.5863  (5487s)\n",
            "[ 52/105] GradBoost       | drop_bucket_large                             | CV=0.7886  Test=0.5875  (5853s)\n",
            "[ 53/105] GradBoost       | drop_bucket_medium                            | CV=0.7888  Test=0.5820  (6208s)\n",
            "[ 54/105] GradBoost       | drop_bucket_small                             | CV=0.7887  Test=0.5868  (6540s)\n",
            "[ 55/105] GradBoost       | only_verb_ratio_comments                      | CV=0.5878  Test=0.6657  (6614s)\n",
            "[ 56/105] GradBoost       | only_text_like_ratio                          | CV=0.6983  Test=0.5750  (6752s)\n",
            "[ 57/105] GradBoost       | only_comments_code_like_ratio_to_total        | CV=0.5733  Test=0.6005  (6844s)\n",
            "[ 58/105] GradBoost       | only_comments_text_like_ratio_to_total        | CV=0.6138  Test=0.6930  (6929s)\n",
            "[ 59/105] GradBoost       | only_comments_code_like_ratio_comments        | CV=0.5717  Test=0.5960  (6996s)\n",
            "[ 60/105] GradBoost       | only_comments_text_like_ratio_comments        | CV=0.5717  Test=0.5960  (7068s)\n",
            "[ 61/105] GradBoost       | only_error_near_eof_ratio                     | CV=0.5939  Test=0.4066  (7121s)\n",
            "[ 62/105] GradBoost       | only_comment_ratio                            | CV=0.6804  Test=0.6574  (7190s)\n",
            "[ 63/105] GradBoost       | prev_2feats                                   | CV=0.6922  Test=0.6455  (7254s)\n",
            "[ 64/105] MLP             | ALL                                           | CV=0.7881  Test=0.5819  (9041s)\n",
            "[ 65/105] MLP             | drop_verb_ratio_comments                      | CV=0.7880  Test=0.5768  (11428s)\n",
            "[ 66/105] MLP             | drop_text_like_ratio                          | CV=0.7739  Test=0.5881  (13706s)\n",
            "[ 67/105] MLP             | drop_comments_code_like_ratio_to_total        | CV=0.7868  Test=0.5752  (14947s)\n",
            "[ 68/105] MLP             | drop_comments_text_like_ratio_to_total        | CV=0.7867  Test=0.5697  (16633s)\n",
            "[ 69/105] MLP             | drop_comments_code_like_ratio_comments        | CV=0.7883  Test=0.5680  (18579s)\n",
            "[ 70/105] MLP             | drop_comments_text_like_ratio_comments        | CV=0.7883  Test=0.5680  (20456s)\n",
            "[ 71/105] MLP             | drop_error_near_eof_ratio                     | CV=0.7499  Test=0.6136  (21549s)\n",
            "[ 72/105] MLP             | drop_comment_ratio                            | CV=0.7789  Test=0.5847  (22610s)\n",
            "[ 73/105] MLP             | drop_bucket_large                             | CV=0.7880  Test=0.5705  (24111s)\n",
            "[ 74/105] MLP             | drop_bucket_medium                            | CV=0.7885  Test=0.5830  (25399s)\n",
            "[ 75/105] MLP             | drop_bucket_small                             | CV=0.7882  Test=0.5776  (26683s)\n",
            "[ 76/105] MLP             | only_verb_ratio_comments                      | CV=0.5878  Test=0.6657  (26821s)\n",
            "[ 77/105] MLP             | only_text_like_ratio                          | CV=0.6971  Test=0.5739  (27095s)\n",
            "[ 78/105] MLP             | only_comments_code_like_ratio_to_total        | CV=0.5726  Test=0.5973  (27536s)\n",
            "[ 79/105] MLP             | only_comments_text_like_ratio_to_total        | CV=0.6133  Test=0.6906  (27820s)\n",
            "[ 80/105] MLP             | only_comments_code_like_ratio_comments        | CV=0.5716  Test=0.5960  (28183s)\n",
            "[ 81/105] MLP             | only_comments_text_like_ratio_comments        | CV=0.5716  Test=0.5960  (28523s)\n",
            "[ 82/105] MLP             | only_error_near_eof_ratio                     | CV=0.5940  Test=0.4066  (29133s)\n",
            "[ 83/105] MLP             | only_comment_ratio                            | CV=0.6803  Test=0.6532  (29500s)\n",
            "[ 84/105] MLP             | prev_2feats                                   | CV=0.6916  Test=0.6447  (29915s)\n",
            "[ 85/105] DeepNN          | ALL                                           | CV=0.7872  Test=0.5907  (30854s)\n",
            "[ 86/105] DeepNN          | drop_verb_ratio_comments                      | CV=0.7874  Test=0.5568  (31811s)\n",
            "[ 87/105] DeepNN          | drop_text_like_ratio                          | CV=0.7738  Test=0.5896  (32873s)\n",
            "[ 88/105] DeepNN          | drop_comments_code_like_ratio_to_total        | CV=0.7866  Test=0.5972  (35229s)\n",
            "[ 89/105] DeepNN          | drop_comments_text_like_ratio_to_total        | CV=0.7860  Test=0.5717  (37124s)\n",
            "[ 90/105] DeepNN          | drop_comments_code_like_ratio_comments        | CV=0.7876  Test=0.5843  (39023s)\n",
            "[ 91/105] DeepNN          | drop_comments_text_like_ratio_comments        | CV=0.7876  Test=0.5843  (40767s)\n",
            "[ 92/105] DeepNN          | drop_error_near_eof_ratio                     | CV=0.7496  Test=0.6237  (42672s)\n",
            "[ 93/105] DeepNN          | drop_comment_ratio                            | CV=0.7787  Test=0.5805  (44172s)\n"
          ]
        }
      ],
      "source": [
        "import warnings, time\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    _HAS_XGB = True\n",
        "except ImportError:\n",
        "    _HAS_XGB = False\n",
        "    print(\"[info] xgboost not installed — using GradientBoostingClassifier\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 1. EXPLORE the processed feature CSVs\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "_N_TEST = 165833  # aligned slice used across all test_add artefacts\n",
        "\n",
        "NUMERIC_CSV_COLS = list(df_feats_train.columns[1:])   # skip 'bucket' (str)\n",
        "df_cross_h = df_cross.head(_N_TEST)\n",
        "\n",
        "print(\"CSV columns:\", list(df_feats_train.columns))\n",
        "print(f\"Train CSV: {df_feats_train.shape}  |  Test-Add CSV: {df_cross_h.shape}\")\n",
        "print(f\"\\nNumeric features from CSV ({len(NUMERIC_CSV_COLS)}):\")\n",
        "for c in NUMERIC_CSV_COLS:\n",
        "    print(f\"  {c:42s} \"\n",
        "          f\"train [{df_feats_train[c].min():.4f} .. {df_feats_train[c].max():.4f}]  \"\n",
        "          f\"test  [{df_cross_h[c].min():.4f} .. {df_cross_h[c].max():.4f}]\")\n",
        "\n",
        "print(f\"\\nBucket distribution (train):\\n{df_feats_train['bucket'].value_counts().to_string()}\")\n",
        "print(f\"\\nBucket distribution (test_add):\\n{df_cross_h['bucket'].value_counts().to_string()}\")\n",
        "\n",
        "print(f\"\\nExtra feature: comment_ratio (computed from raw code)\")\n",
        "print(f\"  X_comments_train: len={len(X_comments_train)}, \"\n",
        "      f\"range [{min(X_comments_train):.4f} .. {max(X_comments_train):.4f}]\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 2. BUILD full feature matrices (7 CSV numeric + comment_ratio + bucket OH)\n",
        "#    Evaluation set → test_add (additional dataset)\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Comment ratios for test_add are already computed in X_test_add_comments\n",
        "assert len(X_test_add_comments) == _N_TEST, \\\n",
        "    f\"X_test_add_comments length {len(X_test_add_comments)} != {_N_TEST}\"\n",
        "\n",
        "# Numeric features from CSV (skip bucket at column 0)\n",
        "tr_numeric = np.nan_to_num(\n",
        "    df_feats_train[NUMERIC_CSV_COLS].values.astype(np.float64))\n",
        "te_numeric = np.nan_to_num(\n",
        "    df_cross_h[NUMERIC_CSV_COLS].values.astype(np.float64))\n",
        "\n",
        "# Comment ratio column\n",
        "tr_comment = np.asarray(X_comments_train, dtype=np.float64).reshape(-1, 1)\n",
        "te_comment = np.asarray(X_test_add_comments, dtype=np.float64).reshape(-1, 1)\n",
        "\n",
        "# Bucket → one-hot (small / medium / large → 3 binary columns)\n",
        "lb = LabelBinarizer()\n",
        "tr_bucket_oh = lb.fit_transform(df_feats_train[\"bucket\"].values)\n",
        "te_bucket_oh = lb.transform(df_cross_h[\"bucket\"].values)\n",
        "bucket_names = [f\"bucket_{c}\" for c in lb.classes_]\n",
        "\n",
        "print(f\"\\nBucket one-hot classes: {list(lb.classes_)} → {tr_bucket_oh.shape[1]} cols\")\n",
        "\n",
        "# Stack everything: 7 numeric + 1 comment_ratio + 3 bucket dummies = 11\n",
        "X_full_tr = np.column_stack([tr_numeric, tr_comment, tr_bucket_oh])\n",
        "X_full_te = np.column_stack([te_numeric, te_comment, te_bucket_oh])\n",
        "\n",
        "ALL_FEAT = NUMERIC_CSV_COLS + [\"comment_ratio\"] + bucket_names\n",
        "y_tr = np.asarray(Y_train, dtype=int)\n",
        "y_te = np.asarray(Y_test_list_add, dtype=int)\n",
        "\n",
        "assert X_full_tr.shape[0] == len(y_tr), \\\n",
        "    f\"Train mismatch: {X_full_tr.shape[0]} vs {len(y_tr)}\"\n",
        "assert X_full_te.shape[0] == len(y_te), \\\n",
        "    f\"Test mismatch: {X_full_te.shape[0]} vs {len(y_te)}\"\n",
        "\n",
        "print(f\"\\nFull feature matrix — Train: {X_full_tr.shape}   Test-Add: {X_full_te.shape}\")\n",
        "print(f\"All features ({len(ALL_FEAT)}): {ALL_FEAT}\")\n",
        "\n",
        "# Impute NaN + global StandardScaler\n",
        "_imp = SimpleImputer(strategy=\"mean\").fit(X_full_tr)\n",
        "X_full_tr = _imp.transform(X_full_tr)\n",
        "X_full_te = _imp.transform(X_full_te)\n",
        "\n",
        "_sc = StandardScaler().fit(X_full_tr)\n",
        "X_tr_s = _sc.transform(X_full_tr)\n",
        "X_te_s = _sc.transform(X_full_te)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 3. FEATURE SUBSETS: ALL, drop-one, only-one (skip bucket dummies alone)\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "feat_subsets = {\"ALL\": list(range(len(ALL_FEAT)))}\n",
        "\n",
        "for i, name in enumerate(ALL_FEAT):\n",
        "    feat_subsets[f\"drop_{name}\"] = [j for j in range(len(ALL_FEAT)) if j != i]\n",
        "\n",
        "for i, name in enumerate(ALL_FEAT):\n",
        "    if not name.startswith(\"bucket_\"):\n",
        "        feat_subsets[f\"only_{name}\"] = [i]\n",
        "\n",
        "# Also test: only the 2 features the previous cells used\n",
        "feat_subsets[\"prev_2feats\"] = [\n",
        "    ALL_FEAT.index(\"verb_ratio_comments\"),\n",
        "    ALL_FEAT.index(\"comment_ratio\"),\n",
        "]\n",
        "\n",
        "print(f\"\\n{len(feat_subsets)} feature subsets defined\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 4. MODEL DEFINITIONS + hyperparameter grids\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "f1m = make_scorer(f1_score, average=\"macro\")\n",
        "\n",
        "model_defs = {}\n",
        "\n",
        "model_defs[\"LogReg\"] = (LogisticRegression, {\n",
        "    \"C\": [0.01, 0.1, 1, 10],\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"solver\": [\"liblinear\"],\n",
        "    \"max_iter\": [5000],\n",
        "    \"random_state\": [42],\n",
        "})\n",
        "\n",
        "model_defs[\"RandomForest\"] = (RandomForestClassifier, {\n",
        "    \"n_estimators\": [100, 200],\n",
        "    \"max_depth\": [2, 5, 10, None],\n",
        "    \"min_samples_leaf\": [1, 5],\n",
        "    \"random_state\": [42],\n",
        "})\n",
        "\n",
        "if _HAS_XGB:\n",
        "    model_defs[\"XGBoost\"] = (XGBClassifier, {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [3, 6],\n",
        "        \"learning_rate\": [0.05, 0.1, 0.3],\n",
        "        \"verbosity\": [0],\n",
        "        \"random_state\": [42],\n",
        "    })\n",
        "else:\n",
        "    model_defs[\"GradBoost\"] = (GradientBoostingClassifier, {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [3, 5],\n",
        "        \"learning_rate\": [0.05, 0.1, 0.3],\n",
        "        \"random_state\": [42],\n",
        "    })\n",
        "\n",
        "model_defs[\"MLP\"] = (MLPClassifier, {\n",
        "    \"hidden_layer_sizes\": [(64, 32), (128, 64)],\n",
        "    \"activation\": [\"relu\", \"tanh\"],\n",
        "    \"alpha\": [1e-4, 1e-3],\n",
        "    \"max_iter\": [500],\n",
        "    \"random_state\": [42],\n",
        "})\n",
        "\n",
        "model_defs[\"DeepNN\"] = (MLPClassifier, {\n",
        "    \"hidden_layer_sizes\": [(128, 64, 32), (256, 128, 64)],\n",
        "    \"activation\": [\"relu\"],\n",
        "    \"alpha\": [1e-4, 1e-3, 1e-2],\n",
        "    \"learning_rate_init\": [1e-3],\n",
        "    \"early_stopping\": [True],\n",
        "    \"max_iter\": [800],\n",
        "    \"random_state\": [42],\n",
        "})\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 5. RUN ALL EXPERIMENTS  (evaluated on test_add)\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "results_rows = []\n",
        "n_total = len(model_defs) * len(feat_subsets)\n",
        "n_done = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for mname, (Cls, pgrid) in model_defs.items():\n",
        "    for sname, cols in feat_subsets.items():\n",
        "        n_done += 1\n",
        "        Xt, Xv = X_tr_s[:, cols], X_te_s[:, cols]\n",
        "\n",
        "        gs = GridSearchCV(\n",
        "            Cls(), pgrid,\n",
        "            scoring=f1m, cv=3, n_jobs=-1, refit=True,\n",
        "        )\n",
        "        gs.fit(Xt, y_tr)\n",
        "\n",
        "        yhat = gs.predict(Xv)\n",
        "        tf1 = f1_score(y_te, yhat, average=\"macro\")\n",
        "\n",
        "        results_rows.append({\n",
        "            \"Model\": mname,\n",
        "            \"Features\": sname,\n",
        "            \"N_feats\": len(cols),\n",
        "            \"CV_F1_macro\": round(gs.best_score_, 5),\n",
        "            \"Test_F1_macro\": round(tf1, 5),\n",
        "            \"Best_Params\": str(gs.best_params_),\n",
        "        })\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"[{n_done:3d}/{n_total}] {mname:15s} | \"\n",
        "              f\"{sname:45s} | CV={gs.best_score_:.4f}  \"\n",
        "              f\"Test={tf1:.4f}  ({elapsed:.0f}s)\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 6. COMPARISON TABLES\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "df_cmp = pd.DataFrame(results_rows).sort_values(\n",
        "    \"Test_F1_macro\", ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 110)\n",
        "print(\"  FEATURE × MODEL COMPARISON  (sorted by Test-Add F1-macro)\")\n",
        "print(\"=\" * 110)\n",
        "with pd.option_context(\n",
        "    \"display.max_colwidth\", 55,\n",
        "    \"display.max_rows\", None,\n",
        "    \"display.width\", 220,\n",
        "):\n",
        "    display(df_cmp[[\"Model\", \"Features\", \"N_feats\",\n",
        "                    \"CV_F1_macro\", \"Test_F1_macro\"]])\n",
        "\n",
        "print(\"\\n── Best configuration per model ──\")\n",
        "best_per = df_cmp.loc[df_cmp.groupby(\"Model\")[\"Test_F1_macro\"].idxmax()]\n",
        "with pd.option_context(\"display.max_colwidth\", 100, \"display.width\", 220):\n",
        "    display(best_per[[\"Model\", \"Features\", \"N_feats\",\n",
        "                      \"CV_F1_macro\", \"Test_F1_macro\", \"Best_Params\"]])\n",
        "\n",
        "print(\"\\n── Best configuration per feature subset ──\")\n",
        "best_per_feat = df_cmp.loc[df_cmp.groupby(\"Features\")[\"Test_F1_macro\"].idxmax()]\n",
        "with pd.option_context(\"display.max_colwidth\", 100, \"display.width\", 220):\n",
        "    display(best_per_feat[[\"Model\", \"Features\", \"N_feats\",\n",
        "                           \"CV_F1_macro\", \"Test_F1_macro\"]].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data prepared for MultinomialNB: Train (300000, 11), Test (165833, 11)\n",
            "Value range: [0.0000, 1.0000]\n",
            "[  1/21] MultinomialNB | ALL                                           | CV=0.7487  Test=0.5942  (8s)\n",
            "[  2/21] MultinomialNB | drop_verb_ratio_comments                      | CV=0.7467  Test=0.5916  (9s)\n",
            "[  3/21] MultinomialNB | drop_text_like_ratio                          | CV=0.7350  Test=0.5956  (9s)\n",
            "[  4/21] MultinomialNB | drop_comments_code_like_ratio_to_total        | CV=0.7492  Test=0.5923  (10s)\n",
            "[  5/21] MultinomialNB | drop_comments_text_like_ratio_to_total        | CV=0.7413  Test=0.5907  (11s)\n",
            "[  6/21] MultinomialNB | drop_comments_code_like_ratio_comments        | CV=0.7455  Test=0.5775  (12s)\n",
            "[  7/21] MultinomialNB | drop_comments_text_like_ratio_comments        | CV=0.7455  Test=0.5775  (13s)\n",
            "[  8/21] MultinomialNB | drop_error_near_eof_ratio                     | CV=0.7241  Test=0.6089  (14s)\n",
            "[  9/21] MultinomialNB | drop_comment_ratio                            | CV=0.7429  Test=0.5923  (15s)\n",
            "[ 10/21] MultinomialNB | drop_bucket_large                             | CV=0.7540  Test=0.5908  (15s)\n",
            "[ 11/21] MultinomialNB | drop_bucket_medium                            | CV=0.7345  Test=0.5682  (16s)\n",
            "[ 12/21] MultinomialNB | drop_bucket_small                             | CV=0.7057  Test=0.5744  (17s)\n",
            "[ 13/21] MultinomialNB | only_verb_ratio_comments                      | CV=0.3435  Test=0.2311  (18s)\n",
            "[ 14/21] MultinomialNB | only_text_like_ratio                          | CV=0.3435  Test=0.2311  (19s)\n",
            "[ 15/21] MultinomialNB | only_comments_code_like_ratio_to_total        | CV=0.3435  Test=0.2311  (19s)\n",
            "[ 16/21] MultinomialNB | only_comments_text_like_ratio_to_total        | CV=0.3435  Test=0.2311  (20s)\n",
            "[ 17/21] MultinomialNB | only_comments_code_like_ratio_comments        | CV=0.3435  Test=0.2311  (21s)\n",
            "[ 18/21] MultinomialNB | only_comments_text_like_ratio_comments        | CV=0.3435  Test=0.2311  (21s)\n",
            "[ 19/21] MultinomialNB | only_error_near_eof_ratio                     | CV=0.3435  Test=0.2311  (22s)\n",
            "[ 20/21] MultinomialNB | only_comment_ratio                            | CV=0.3435  Test=0.2311  (22s)\n",
            "[ 21/21] MultinomialNB | prev_2feats                                   | CV=0.5696  Test=0.5208  (23s)\n",
            "\n",
            "==========================================================================================\n",
            "  MULTINOMIAL NAIVE BAYES RESULTS  (sorted by Test-Add F1-macro)\n",
            "==========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>N_feats</th>\n",
              "      <th>CV_F1_macro</th>\n",
              "      <th>Test_F1_macro</th>\n",
              "      <th>Best_Params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>drop_error_near_eof_ratio</td>\n",
              "      <td>10</td>\n",
              "      <td>0.72413</td>\n",
              "      <td>0.60886</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>drop_text_like_ratio</td>\n",
              "      <td>10</td>\n",
              "      <td>0.73496</td>\n",
              "      <td>0.59561</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ALL</td>\n",
              "      <td>11</td>\n",
              "      <td>0.74873</td>\n",
              "      <td>0.59417</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>drop_comment_ratio</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74293</td>\n",
              "      <td>0.59233</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>drop_comments_code_like_ratio_to_total</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74923</td>\n",
              "      <td>0.59231</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>drop_verb_ratio_comments</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74668</td>\n",
              "      <td>0.59159</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>drop_bucket_large</td>\n",
              "      <td>10</td>\n",
              "      <td>0.75403</td>\n",
              "      <td>0.59080</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>drop_comments_text_like_ratio_to_total</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74129</td>\n",
              "      <td>0.59066</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>drop_comments_text_like_ratio_comments</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74555</td>\n",
              "      <td>0.57751</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>drop_comments_code_like_ratio_comments</td>\n",
              "      <td>10</td>\n",
              "      <td>0.74555</td>\n",
              "      <td>0.57751</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>drop_bucket_small</td>\n",
              "      <td>10</td>\n",
              "      <td>0.70569</td>\n",
              "      <td>0.57437</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>drop_bucket_medium</td>\n",
              "      <td>10</td>\n",
              "      <td>0.73445</td>\n",
              "      <td>0.56817</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>prev_2feats</td>\n",
              "      <td>2</td>\n",
              "      <td>0.56961</td>\n",
              "      <td>0.52078</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>only_verb_ratio_comments</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>only_text_like_ratio</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>only_comments_text_like_ratio_to_total</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>only_comments_code_like_ratio_to_total</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>only_comments_code_like_ratio_comments</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>only_comments_text_like_ratio_comments</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>only_error_near_eof_ratio</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>only_comment_ratio</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34346</td>\n",
              "      <td>0.23108</td>\n",
              "      <td>{'alpha': 0.001, 'fit_prior': True}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Features  N_feats  CV_F1_macro  Test_F1_macro                           Best_Params\n",
              "0                drop_error_near_eof_ratio       10      0.72413        0.60886  {'alpha': 0.001, 'fit_prior': False}\n",
              "1                     drop_text_like_ratio       10      0.73496        0.59561  {'alpha': 0.001, 'fit_prior': False}\n",
              "2                                      ALL       11      0.74873        0.59417  {'alpha': 0.001, 'fit_prior': False}\n",
              "3                       drop_comment_ratio       10      0.74293        0.59233  {'alpha': 0.001, 'fit_prior': False}\n",
              "4   drop_comments_code_like_ratio_to_total       10      0.74923        0.59231  {'alpha': 0.001, 'fit_prior': False}\n",
              "5                 drop_verb_ratio_comments       10      0.74668        0.59159  {'alpha': 0.001, 'fit_prior': False}\n",
              "6                        drop_bucket_large       10      0.75403        0.59080  {'alpha': 0.001, 'fit_prior': False}\n",
              "7   drop_comments_text_like_ratio_to_total       10      0.74129        0.59066  {'alpha': 0.001, 'fit_prior': False}\n",
              "8   drop_comments_text_like_ratio_comments       10      0.74555        0.57751  {'alpha': 0.001, 'fit_prior': False}\n",
              "9   drop_comments_code_like_ratio_comments       10      0.74555        0.57751  {'alpha': 0.001, 'fit_prior': False}\n",
              "10                       drop_bucket_small       10      0.70569        0.57437  {'alpha': 0.001, 'fit_prior': False}\n",
              "11                      drop_bucket_medium       10      0.73445        0.56817  {'alpha': 0.001, 'fit_prior': False}\n",
              "12                             prev_2feats        2      0.56961        0.52078  {'alpha': 0.001, 'fit_prior': False}\n",
              "13                only_verb_ratio_comments        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "14                    only_text_like_ratio        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "15  only_comments_text_like_ratio_to_total        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "16  only_comments_code_like_ratio_to_total        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "17  only_comments_code_like_ratio_comments        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "18  only_comments_text_like_ratio_comments        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "19               only_error_near_eof_ratio        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}\n",
              "20                      only_comment_ratio        1      0.34346        0.23108   {'alpha': 0.001, 'fit_prior': True}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best configuration: drop_error_near_eof_ratio with Test F1 = 0.6089\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# MULTINOMIAL NAIVE BAYES EXPERIMENT\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# Note: MultinomialNB requires non-negative features, so we use MinMaxScaler\n",
        "\n",
        "import warnings, time\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Prepare data with MinMaxScaler (ensures non-negative values for MultinomialNB)\n",
        "_imp_nb = SimpleImputer(strategy=\"mean\").fit(X_full_tr)\n",
        "X_tr_imp = _imp_nb.transform(X_full_tr)\n",
        "X_te_imp = _imp_nb.transform(X_full_te)\n",
        "\n",
        "_sc_nb = MinMaxScaler().fit(X_tr_imp)\n",
        "X_tr_nb = _sc_nb.transform(X_tr_imp)\n",
        "X_te_nb = _sc_nb.transform(X_te_imp)\n",
        "\n",
        "print(f\"Data prepared for MultinomialNB: Train {X_tr_nb.shape}, Test {X_te_nb.shape}\")\n",
        "print(f\"Value range: [{X_tr_nb.min():.4f}, {X_tr_nb.max():.4f}]\")\n",
        "\n",
        "# Feature subsets (reuse from previous experiment)\n",
        "feat_subsets_nb = {\"ALL\": list(range(len(ALL_FEAT)))}\n",
        "for i, name in enumerate(ALL_FEAT):\n",
        "    feat_subsets_nb[f\"drop_{name}\"] = [j for j in range(len(ALL_FEAT)) if j != i]\n",
        "for i, name in enumerate(ALL_FEAT):\n",
        "    if not name.startswith(\"bucket_\"):\n",
        "        feat_subsets_nb[f\"only_{name}\"] = [i]\n",
        "feat_subsets_nb[\"prev_2feats\"] = [\n",
        "    ALL_FEAT.index(\"verb_ratio_comments\"),\n",
        "    ALL_FEAT.index(\"comment_ratio\"),\n",
        "]\n",
        "\n",
        "# Model definition with hyperparameter grid\n",
        "f1m = make_scorer(f1_score, average=\"macro\")\n",
        "mnb_grid = {\n",
        "    \"alpha\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
        "    \"fit_prior\": [True, False],\n",
        "}\n",
        "\n",
        "# Run experiments\n",
        "results_nb = []\n",
        "n_total = len(feat_subsets_nb)\n",
        "n_done = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for sname, cols in feat_subsets_nb.items():\n",
        "    n_done += 1\n",
        "    Xt, Xv = X_tr_nb[:, cols], X_te_nb[:, cols]\n",
        "    \n",
        "    gs = GridSearchCV(\n",
        "        MultinomialNB(), mnb_grid,\n",
        "        scoring=f1m, cv=3, n_jobs=-1, refit=True,\n",
        "    )\n",
        "    gs.fit(Xt, y_tr)\n",
        "    \n",
        "    yhat = gs.predict(Xv)\n",
        "    tf1 = f1_score(y_te, yhat, average=\"macro\")\n",
        "    \n",
        "    results_nb.append({\n",
        "        \"Model\": \"MultinomialNB\",\n",
        "        \"Features\": sname,\n",
        "        \"N_feats\": len(cols),\n",
        "        \"CV_F1_macro\": round(gs.best_score_, 5),\n",
        "        \"Test_F1_macro\": round(tf1, 5),\n",
        "        \"Best_Params\": str(gs.best_params_),\n",
        "    })\n",
        "    \n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"[{n_done:3d}/{n_total}] MultinomialNB | \"\n",
        "          f\"{sname:45s} | CV={gs.best_score_:.4f}  \"\n",
        "          f\"Test={tf1:.4f}  ({elapsed:.0f}s)\")\n",
        "\n",
        "# Results table\n",
        "df_nb = pd.DataFrame(results_nb).sort_values(\n",
        "    \"Test_F1_macro\", ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"  MULTINOMIAL NAIVE BAYES RESULTS  (sorted by Test-Add F1-macro)\")\n",
        "print(\"=\" * 90)\n",
        "with pd.option_context(\n",
        "    \"display.max_colwidth\", 55,\n",
        "    \"display.max_rows\", None,\n",
        "    \"display.width\", 200,\n",
        "):\n",
        "    display(df_nb[[\"Features\", \"N_feats\", \"CV_F1_macro\", \"Test_F1_macro\", \"Best_Params\"]])\n",
        "\n",
        "print(f\"\\nBest configuration: {df_nb.iloc[0]['Features']} with Test F1 = {df_nb.iloc[0]['Test_F1_macro']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results exported to: data/results_all_models.xlsx\n",
            "  - 126 total experiments\n",
            "  - Top 4 results highlighted\n",
            "\n",
            "======================================================================\n",
            "  TOP 4 RESULTS\n",
            "======================================================================\n",
            "       Model                               Features  Test_F1_macro\n",
            "RandomForest only_comments_text_like_ratio_to_total         0.6932\n",
            "     XGBoost only_comments_text_like_ratio_to_total         0.6930\n",
            "         MLP only_comments_text_like_ratio_to_total         0.6906\n",
            "      LogReg only_comments_text_like_ratio_to_total         0.6900\n",
            "\n",
            "======================================================================\n",
            "  STACKING ENSEMBLE (30k subset)\n",
            "======================================================================\n",
            "  Training subset: 30,000 samples\n",
            "\n",
            "  Test F1-macro: 0.6911\n",
            "  Training time: 14.0s\n",
            "\n",
            "  Best single model (RF): 0.6932\n",
            "  Stacking Ensemble:      0.6911\n",
            "  Delta: -0.0021\n",
            "\n",
            "Ensemble saved to: data/results_all_models.xlsx (sheet: 'Stacking Ensemble')\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# EXPORT RESULTS TO EXCEL & ENSEMBLE EXPERIMENT\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1. COMPLETE RESULTS DATA (hardcoded from experiments)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "all_results = [\n",
        "    # LogReg results\n",
        "    (\"LogReg\", \"only_comments_text_like_ratio_to_total\", 1, 0.6131, 0.6900),\n",
        "    (\"LogReg\", \"only_verb_ratio_comments\", 1, 0.5877, 0.6658),\n",
        "    (\"LogReg\", \"only_comment_ratio\", 1, 0.6735, 0.6497),\n",
        "    (\"LogReg\", \"prev_2feats\", 2, 0.6803, 0.6464),\n",
        "    (\"LogReg\", \"drop_error_near_eof_ratio\", 10, 0.7341, 0.6209),\n",
        "    (\"LogReg\", \"ALL\", 11, 0.7618, 0.6003),\n",
        "    (\"LogReg\", \"drop_comments_code_like_ratio_comments\", 10, 0.7618, 0.6003),\n",
        "    (\"LogReg\", \"drop_comments_text_like_ratio_comments\", 10, 0.7618, 0.6003),\n",
        "    (\"LogReg\", \"drop_bucket_medium\", 10, 0.7617, 0.6003),\n",
        "    (\"LogReg\", \"drop_bucket_small\", 10, 0.7618, 0.6003),\n",
        "    (\"LogReg\", \"drop_text_like_ratio\", 10, 0.7487, 0.6002),\n",
        "    (\"LogReg\", \"drop_bucket_large\", 10, 0.7618, 0.6002),\n",
        "    (\"LogReg\", \"only_comments_code_like_ratio_to_total\", 1, 0.5725, 0.5969),\n",
        "    (\"LogReg\", \"drop_verb_ratio_comments\", 10, 0.7621, 0.5956),\n",
        "    (\"LogReg\", \"drop_comment_ratio\", 10, 0.7577, 0.5952),\n",
        "    (\"LogReg\", \"only_comments_code_like_ratio_comments\", 1, 0.5712, 0.5938),\n",
        "    (\"LogReg\", \"only_comments_text_like_ratio_comments\", 1, 0.5712, 0.5938),\n",
        "    (\"LogReg\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7623, 0.5989),\n",
        "    (\"LogReg\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7584, 0.5869),\n",
        "    (\"LogReg\", \"only_text_like_ratio\", 1, 0.6983, 0.5701),\n",
        "    (\"LogReg\", \"only_error_near_eof_ratio\", 1, 0.5907, 0.4074),\n",
        "    # RandomForest results\n",
        "    (\"RandomForest\", \"only_comments_text_like_ratio_to_total\", 1, 0.6139, 0.6932),\n",
        "    (\"RandomForest\", \"only_verb_ratio_comments\", 1, 0.5878, 0.6657),\n",
        "    (\"RandomForest\", \"only_comment_ratio\", 1, 0.6804, 0.6593),\n",
        "    (\"RandomForest\", \"prev_2feats\", 2, 0.6921, 0.6441),\n",
        "    (\"RandomForest\", \"drop_error_near_eof_ratio\", 10, 0.7509, 0.6201),\n",
        "    (\"RandomForest\", \"drop_text_like_ratio\", 10, 0.7747, 0.5896),\n",
        "    (\"RandomForest\", \"drop_comment_ratio\", 10, 0.7800, 0.5887),\n",
        "    (\"RandomForest\", \"ALL\", 11, 0.7893, 0.5829),\n",
        "    (\"RandomForest\", \"drop_verb_ratio_comments\", 10, 0.7890, 0.5826),\n",
        "    (\"RandomForest\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7880, 0.5824),\n",
        "    (\"RandomForest\", \"drop_bucket_medium\", 10, 0.7893, 0.5821),\n",
        "    (\"RandomForest\", \"drop_bucket_small\", 10, 0.7892, 0.5815),\n",
        "    (\"RandomForest\", \"drop_comments_code_like_ratio_comments\", 10, 0.7893, 0.5796),\n",
        "    (\"RandomForest\", \"drop_comments_text_like_ratio_comments\", 10, 0.7893, 0.5796),\n",
        "    (\"RandomForest\", \"drop_bucket_large\", 10, 0.7889, 0.5781),\n",
        "    (\"RandomForest\", \"only_comments_code_like_ratio_comments\", 1, 0.5716, 0.5960),\n",
        "    (\"RandomForest\", \"only_comments_text_like_ratio_comments\", 1, 0.5716, 0.5960),\n",
        "    (\"RandomForest\", \"only_comments_code_like_ratio_to_total\", 1, 0.5730, 0.5991),\n",
        "    (\"RandomForest\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7878, 0.5689),\n",
        "    (\"RandomForest\", \"only_text_like_ratio\", 1, 0.6989, 0.5733),\n",
        "    (\"RandomForest\", \"only_error_near_eof_ratio\", 1, 0.5940, 0.4066),\n",
        "    # XGBoost results\n",
        "    (\"XGBoost\", \"only_comments_text_like_ratio_to_total\", 1, 0.6138, 0.6930),\n",
        "    (\"XGBoost\", \"only_verb_ratio_comments\", 1, 0.5878, 0.6657),\n",
        "    (\"XGBoost\", \"only_comment_ratio\", 1, 0.6804, 0.6574),\n",
        "    (\"XGBoost\", \"prev_2feats\", 2, 0.6922, 0.6455),\n",
        "    (\"XGBoost\", \"drop_error_near_eof_ratio\", 10, 0.7511, 0.6213),\n",
        "    (\"XGBoost\", \"drop_text_like_ratio\", 10, 0.7745, 0.5901),\n",
        "    (\"XGBoost\", \"drop_bucket_large\", 10, 0.7886, 0.5875),\n",
        "    (\"XGBoost\", \"drop_bucket_small\", 10, 0.7887, 0.5868),\n",
        "    (\"XGBoost\", \"drop_comment_ratio\", 10, 0.7796, 0.5863),\n",
        "    (\"XGBoost\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7878, 0.5837),\n",
        "    (\"XGBoost\", \"drop_verb_ratio_comments\", 10, 0.7887, 0.5829),\n",
        "    (\"XGBoost\", \"drop_bucket_medium\", 10, 0.7888, 0.5820),\n",
        "    (\"XGBoost\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7875, 0.5813),\n",
        "    (\"XGBoost\", \"ALL\", 11, 0.7886, 0.5809),\n",
        "    (\"XGBoost\", \"drop_comments_code_like_ratio_comments\", 10, 0.7886, 0.5809),\n",
        "    (\"XGBoost\", \"drop_comments_text_like_ratio_comments\", 10, 0.7886, 0.5809),\n",
        "    (\"XGBoost\", \"only_comments_code_like_ratio_to_total\", 1, 0.5733, 0.6005),\n",
        "    (\"XGBoost\", \"only_comments_code_like_ratio_comments\", 1, 0.5717, 0.5960),\n",
        "    (\"XGBoost\", \"only_comments_text_like_ratio_comments\", 1, 0.5717, 0.5960),\n",
        "    (\"XGBoost\", \"only_text_like_ratio\", 1, 0.6983, 0.5750),\n",
        "    (\"XGBoost\", \"only_error_near_eof_ratio\", 1, 0.5939, 0.4066),\n",
        "    # MLP results\n",
        "    (\"MLP\", \"only_comments_text_like_ratio_to_total\", 1, 0.6133, 0.6906),\n",
        "    (\"MLP\", \"only_verb_ratio_comments\", 1, 0.5878, 0.6657),\n",
        "    (\"MLP\", \"only_comment_ratio\", 1, 0.6803, 0.6532),\n",
        "    (\"MLP\", \"prev_2feats\", 2, 0.6916, 0.6447),\n",
        "    (\"MLP\", \"drop_error_near_eof_ratio\", 10, 0.7503, 0.6198),\n",
        "    (\"MLP\", \"drop_text_like_ratio\", 10, 0.7739, 0.5881),\n",
        "    (\"MLP\", \"drop_comment_ratio\", 10, 0.7789, 0.5847),\n",
        "    (\"MLP\", \"drop_bucket_medium\", 10, 0.7885, 0.5830),\n",
        "    (\"MLP\", \"ALL\", 11, 0.7881, 0.5819),\n",
        "    (\"MLP\", \"drop_bucket_small\", 10, 0.7882, 0.5776),\n",
        "    (\"MLP\", \"drop_verb_ratio_comments\", 10, 0.7880, 0.5768),\n",
        "    (\"MLP\", \"drop_bucket_large\", 10, 0.7880, 0.5705),\n",
        "    (\"MLP\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7867, 0.5697),\n",
        "    (\"MLP\", \"drop_comments_code_like_ratio_comments\", 10, 0.7883, 0.5680),\n",
        "    (\"MLP\", \"drop_comments_text_like_ratio_comments\", 10, 0.7883, 0.5680),\n",
        "    (\"MLP\", \"only_comments_code_like_ratio_to_total\", 1, 0.5726, 0.5973),\n",
        "    (\"MLP\", \"only_comments_code_like_ratio_comments\", 1, 0.5716, 0.5960),\n",
        "    (\"MLP\", \"only_comments_text_like_ratio_comments\", 1, 0.5716, 0.5960),\n",
        "    (\"MLP\", \"only_text_like_ratio\", 1, 0.6972, 0.5698),\n",
        "    (\"MLP\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7870, 0.5665),\n",
        "    (\"MLP\", \"only_error_near_eof_ratio\", 1, 0.5940, 0.4066),\n",
        "    # DeepNN results\n",
        "    (\"DeepNN\", \"only_comments_text_like_ratio_to_total\", 1, 0.6129, 0.6878),\n",
        "    (\"DeepNN\", \"only_verb_ratio_comments\", 1, 0.5875, 0.6641),\n",
        "    (\"DeepNN\", \"only_comment_ratio\", 1, 0.6798, 0.6508),\n",
        "    (\"DeepNN\", \"prev_2feats\", 2, 0.6911, 0.6421),\n",
        "    (\"DeepNN\", \"drop_error_near_eof_ratio\", 10, 0.7496, 0.6237),\n",
        "    (\"DeepNN\", \"drop_text_like_ratio\", 10, 0.7738, 0.5896),\n",
        "    (\"DeepNN\", \"drop_bucket_large\", 10, 0.7879, 0.5862),\n",
        "    (\"DeepNN\", \"drop_comments_code_like_ratio_comments\", 10, 0.7876, 0.5843),\n",
        "    (\"DeepNN\", \"drop_comments_text_like_ratio_comments\", 10, 0.7876, 0.5843),\n",
        "    (\"DeepNN\", \"drop_bucket_medium\", 10, 0.7881, 0.5835),\n",
        "    (\"DeepNN\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7863, 0.5831),\n",
        "    (\"DeepNN\", \"ALL\", 11, 0.7872, 0.5811),\n",
        "    (\"DeepNN\", \"drop_comment_ratio\", 10, 0.7787, 0.5805),\n",
        "    (\"DeepNN\", \"drop_verb_ratio_comments\", 10, 0.7874, 0.5568),\n",
        "    (\"DeepNN\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7860, 0.5717),\n",
        "    (\"DeepNN\", \"only_comments_code_like_ratio_to_total\", 1, 0.5721, 0.5981),\n",
        "    (\"DeepNN\", \"only_comments_code_like_ratio_comments\", 1, 0.5711, 0.5948),\n",
        "    (\"DeepNN\", \"only_comments_text_like_ratio_comments\", 1, 0.5711, 0.5948),\n",
        "    (\"DeepNN\", \"only_text_like_ratio\", 1, 0.6965, 0.5721),\n",
        "    (\"DeepNN\", \"drop_bucket_small\", 10, 0.7875, 0.5741),\n",
        "    (\"DeepNN\", \"only_error_near_eof_ratio\", 1, 0.5938, 0.4058),\n",
        "    # MultinomialNB results\n",
        "    (\"MultinomialNB\", \"drop_error_near_eof_ratio\", 10, 0.7241, 0.6089),\n",
        "    (\"MultinomialNB\", \"drop_text_like_ratio\", 10, 0.7350, 0.5956),\n",
        "    (\"MultinomialNB\", \"ALL\", 11, 0.7487, 0.5942),\n",
        "    (\"MultinomialNB\", \"drop_comment_ratio\", 10, 0.7429, 0.5923),\n",
        "    (\"MultinomialNB\", \"drop_comments_code_like_ratio_to_total\", 10, 0.7492, 0.5923),\n",
        "    (\"MultinomialNB\", \"drop_verb_ratio_comments\", 10, 0.7467, 0.5916),\n",
        "    (\"MultinomialNB\", \"drop_bucket_large\", 10, 0.7540, 0.5908),\n",
        "    (\"MultinomialNB\", \"drop_comments_text_like_ratio_to_total\", 10, 0.7413, 0.5907),\n",
        "    (\"MultinomialNB\", \"drop_comments_text_like_ratio_comments\", 10, 0.7455, 0.5775),\n",
        "    (\"MultinomialNB\", \"drop_comments_code_like_ratio_comments\", 10, 0.7455, 0.5775),\n",
        "    (\"MultinomialNB\", \"drop_bucket_small\", 10, 0.7057, 0.5744),\n",
        "    (\"MultinomialNB\", \"drop_bucket_medium\", 10, 0.7345, 0.5682),\n",
        "    (\"MultinomialNB\", \"prev_2feats\", 2, 0.5696, 0.5208),\n",
        "    (\"MultinomialNB\", \"only_verb_ratio_comments\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_text_like_ratio\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_comments_text_like_ratio_to_total\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_comments_code_like_ratio_to_total\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_comments_code_like_ratio_comments\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_comments_text_like_ratio_comments\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_error_near_eof_ratio\", 1, 0.3435, 0.2311),\n",
        "    (\"MultinomialNB\", \"only_comment_ratio\", 1, 0.3435, 0.2311),\n",
        "]\n",
        "\n",
        "df_all = pd.DataFrame(all_results, columns=[\"Model\", \"Features\", \"N_feats\", \"CV_F1_macro\", \"Test_F1_macro\"])\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2. EXPORT TO EXCEL\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "model_order = [\"LogReg\", \"RandomForest\", \"XGBoost\", \"MLP\", \"DeepNN\", \"MultinomialNB\"]\n",
        "df_all[\"Model_Order\"] = df_all[\"Model\"].apply(lambda x: model_order.index(x) if x in model_order else 99)\n",
        "df_sorted = df_all.sort_values([\"Model_Order\", \"Test_F1_macro\"], ascending=[True, False]).drop(columns=[\"Model_Order\"])\n",
        "\n",
        "excel_path = \"data/results_all_models.xlsx\"\n",
        "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
        "    df_sorted.reset_index(drop=True).to_excel(writer, sheet_name=\"All Results\", index=False)\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets[\"All Results\"]\n",
        "    \n",
        "    header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
        "    header_font = Font(bold=True, color=\"FFFFFF\")\n",
        "    thin_border = Border(left=Side(style=\"thin\"), right=Side(style=\"thin\"),\n",
        "                         top=Side(style=\"thin\"), bottom=Side(style=\"thin\"))\n",
        "    \n",
        "    for col in range(1, 6):\n",
        "        cell = worksheet.cell(row=1, column=col)\n",
        "        cell.fill = header_fill\n",
        "        cell.font = header_font\n",
        "        cell.alignment = Alignment(horizontal=\"center\")\n",
        "        cell.border = thin_border\n",
        "    \n",
        "    df_sorted_reset = df_sorted.reset_index(drop=True)\n",
        "    top4_global = df_all.nlargest(4, \"Test_F1_macro\")\n",
        "    bold_font = Font(bold=True)\n",
        "    highlight_fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "    \n",
        "    for idx, row in df_sorted_reset.iterrows():\n",
        "        is_top4 = any(\n",
        "            (top4_global[\"Model\"] == row[\"Model\"]).values & \n",
        "            (top4_global[\"Features\"] == row[\"Features\"]).values &\n",
        "            (abs(top4_global[\"Test_F1_macro\"] - row[\"Test_F1_macro\"]) < 0.0001).values\n",
        "        )\n",
        "        excel_row = idx + 2\n",
        "        for col in range(1, 6):\n",
        "            cell = worksheet.cell(row=excel_row, column=col)\n",
        "            cell.border = thin_border\n",
        "            if is_top4:\n",
        "                cell.font = bold_font\n",
        "                cell.fill = highlight_fill\n",
        "    \n",
        "    column_widths = [15, 45, 10, 15, 15]\n",
        "    for i, width in enumerate(column_widths):\n",
        "        worksheet.column_dimensions[chr(65 + i)].width = width\n",
        "\n",
        "print(f\"Results exported to: {excel_path}\")\n",
        "print(f\"  - {len(df_sorted)} total experiments\")\n",
        "print(f\"  - Top 4 results highlighted\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"  TOP 4 RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "print(df_all.nlargest(4, \"Test_F1_macro\")[[\"Model\", \"Features\", \"Test_F1_macro\"]].to_string(index=False))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3. STACKING ENSEMBLE\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"  STACKING ENSEMBLE (30k subset)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_feat_idx = [ALL_FEAT.index(\"comments_text_like_ratio_to_total\")]\n",
        "X_tr_best = X_full_tr[:, best_feat_idx]\n",
        "X_te_best = X_full_te[:, best_feat_idx]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "TRAIN_SUBSET = 30000\n",
        "X_tr_sub, _, y_tr_sub, _ = train_test_split(X_tr_best, y_tr, train_size=TRAIN_SUBSET, stratify=y_tr, random_state=42)\n",
        "print(f\"  Training subset: {TRAIN_SUBSET:,} samples\")\n",
        "\n",
        "imp_ens = SimpleImputer(strategy=\"mean\").fit(X_tr_sub)\n",
        "X_tr_ens = imp_ens.transform(X_tr_sub)\n",
        "X_te_ens = imp_ens.transform(X_te_best)\n",
        "\n",
        "sc_ens = StandardScaler().fit(X_tr_ens)\n",
        "X_tr_ens_sc = sc_ens.transform(X_tr_ens)\n",
        "X_te_ens_sc = sc_ens.transform(X_te_ens)\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "base_estimators = [\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)),\n",
        "    (\"gb\", GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
        "    (\"mlp\", MLPClassifier(hidden_layer_sizes=(32,), max_iter=200, random_state=42)),\n",
        "    (\"lr\", LogisticRegression(max_iter=500, random_state=42)),\n",
        "    (\"et\", ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1)),\n",
        "]\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=500, random_state=42),\n",
        "    cv=2, n_jobs=-1, passthrough=False,\n",
        ")\n",
        "\n",
        "import time\n",
        "t0 = time.time()\n",
        "stacking_clf.fit(X_tr_ens_sc, y_tr_sub)\n",
        "train_time = time.time() - t0\n",
        "\n",
        "y_pred_stack = stacking_clf.predict(X_te_ens_sc)\n",
        "f1_stack = f1_score(y_te, y_pred_stack, average=\"macro\")\n",
        "\n",
        "print(f\"\\n  Test F1-macro: {f1_stack:.4f}\")\n",
        "print(f\"  Training time: {train_time:.1f}s\")\n",
        "print(f\"\\n  Best single model (RF): 0.6932\")\n",
        "print(f\"  Stacking Ensemble:      {f1_stack:.4f}\")\n",
        "improvement = f1_stack - 0.6932\n",
        "print(f\"  Delta: {improvement:+.4f}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4. SAVE ENSEMBLE TO EXCEL\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "ensemble_data = [{\n",
        "    \"Ensemble\": \"Stacking (RF+GB+MLP+LR+ET -> LogReg)\",\n",
        "    \"Feature\": \"comments_text_like_ratio_to_total\",\n",
        "    \"Train_Subset\": TRAIN_SUBSET,\n",
        "    \"Test_F1_macro\": round(f1_stack, 4),\n",
        "    \"Training_Time_s\": round(train_time, 1),\n",
        "    \"Best_Single_Model\": \"RandomForest\",\n",
        "    \"Best_Single_F1\": 0.6932,\n",
        "    \"Delta\": round(improvement, 4),\n",
        "}]\n",
        "df_ensemble = pd.DataFrame(ensemble_data)\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
        "    df_ensemble.to_excel(writer, sheet_name=\"Stacking Ensemble\", index=False)\n",
        "    ws = writer.sheets[\"Stacking Ensemble\"]\n",
        "    for col in range(1, 9):\n",
        "        cell = ws.cell(row=1, column=col)\n",
        "        cell.fill = header_fill\n",
        "        cell.font = header_font\n",
        "        cell.alignment = Alignment(horizontal=\"center\")\n",
        "    ws.column_dimensions['A'].width = 40\n",
        "    ws.column_dimensions['B'].width = 40\n",
        "    for c in 'CDEFGH':\n",
        "        ws.column_dimensions[c].width = 15\n",
        "\n",
        "print(f\"\\nEnsemble saved to: {excel_path} (sheet: 'Stacking Ensemble')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected feature indices: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10]\n",
            "Selected features (10): ['verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total', 'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments', 'comments_text_like_ratio_comments', 'comment_ratio', 'bucket_large', 'bucket_medium', 'bucket_small']\n",
            "\n",
            "Model performance - Test F1-macro: 0.6217\n",
            "Model bundle saved to: models/ai_detector.joblib\n",
            "\n",
            "Loaded model features: ['verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total', 'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments', 'comments_text_like_ratio_comments', 'comment_ratio', 'bucket_large', 'bucket_medium', 'bucket_small']\n",
            "Loaded model test F1: 0.6217\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# TRAIN AND SAVE BEST MULTI-FEATURE MODEL FOR STREAMLIT APP\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Features for drop_error_near_eof_ratio (10 features - excludes error_near_eof_ratio)\n",
        "# Based on ALL_FEAT = ['verb_ratio_comments', 'text_like_ratio', 'comments_code_like_ratio_to_total',\n",
        "#                      'comments_text_like_ratio_to_total', 'comments_code_like_ratio_comments',\n",
        "#                      'comments_text_like_ratio_comments', 'error_near_eof_ratio', 'comment_ratio',\n",
        "#                      'bucket_large', 'bucket_medium', 'bucket_small']\n",
        "\n",
        "# Feature indices (excluding error_near_eof_ratio at index 6)\n",
        "SELECTED_FEATURES = [\n",
        "    'verb_ratio_comments',           # 0\n",
        "    'text_like_ratio',               # 1\n",
        "    'comments_code_like_ratio_to_total',  # 2\n",
        "    'comments_text_like_ratio_to_total',  # 3\n",
        "    'comments_code_like_ratio_comments',  # 4\n",
        "    'comments_text_like_ratio_comments',  # 5\n",
        "    # 'error_near_eof_ratio',        # 6 - EXCLUDED\n",
        "    'comment_ratio',                 # 7\n",
        "    'bucket_large',                  # 8\n",
        "    'bucket_medium',                 # 9\n",
        "    'bucket_small',                  # 10\n",
        "]\n",
        "\n",
        "# Get indices in ALL_FEAT\n",
        "selected_indices = [ALL_FEAT.index(f) for f in SELECTED_FEATURES]\n",
        "print(f\"Selected feature indices: {selected_indices}\")\n",
        "print(f\"Selected features ({len(SELECTED_FEATURES)}): {SELECTED_FEATURES}\")\n",
        "\n",
        "# Extract feature subset\n",
        "X_tr_selected = X_full_tr[:, selected_indices]\n",
        "X_te_selected = X_full_te[:, selected_indices]\n",
        "\n",
        "# Preprocessing: Imputer + Scaler\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_tr_imp = imputer.fit_transform(X_tr_selected)\n",
        "X_te_imp = imputer.transform(X_te_selected)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_tr_scaled = scaler.fit_transform(X_tr_imp)\n",
        "X_te_scaled = scaler.transform(X_te_imp)\n",
        "\n",
        "# Train LogReg with best params (from GridSearchCV results)\n",
        "model = LogisticRegression(\n",
        "    C=10,\n",
        "    penalty='l2',\n",
        "    solver='liblinear',\n",
        "    max_iter=5000,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_tr_scaled, y_tr)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_te_scaled)\n",
        "f1 = f1_score(y_te, y_pred, average='macro')\n",
        "print(f\"\\nModel performance - Test F1-macro: {f1:.4f}\")\n",
        "\n",
        "# Save model bundle\n",
        "model_bundle = {\n",
        "    'model': model,\n",
        "    'scaler': scaler,\n",
        "    'imputer': imputer,\n",
        "    'features': SELECTED_FEATURES,\n",
        "    'feature_indices': selected_indices,\n",
        "    'test_f1_macro': f1,\n",
        "}\n",
        "\n",
        "save_path = 'models/ai_detector.joblib'\n",
        "joblib.dump(model_bundle, save_path)\n",
        "print(f\"Model bundle saved to: {save_path}\")\n",
        "\n",
        "# Verify load\n",
        "loaded = joblib.load(save_path)\n",
        "print(f\"\\nLoaded model features: {loaded['features']}\")\n",
        "print(f\"Loaded model test F1: {loaded['test_f1_macro']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Results Summary\n",
        "\n",
        "### All Model x Feature Subset Results (Top Results by Test F1-macro)\n",
        "\n",
        "| Model | Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|-------|----------|---------|-------------|---------------|\n",
        "| RandomForest | only_comments_text_like_ratio_to_total | 1 | 0.6139 | **0.6932** |\n",
        "| XGBoost | only_comments_text_like_ratio_to_total | 1 | 0.6138 | **0.6930** |\n",
        "| Stacking Ensemble | only_comments_text_like_ratio_to_total | 1 | - | 0.6911 |\n",
        "| MLP | only_comments_text_like_ratio_to_total | 1 | 0.6133 | **0.6906** |\n",
        "| LogReg | only_comments_text_like_ratio_to_total | 1 | 0.6131 | **0.6900** |\n",
        "| DeepNN | only_comments_text_like_ratio_to_total | 1 | 0.6129 | 0.6878 |\n",
        "| LogReg | only_verb_ratio_comments | 1 | 0.5877 | 0.6658 |\n",
        "| RandomForest | only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| XGBoost | only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| MLP | only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| DeepNN | only_verb_ratio_comments | 1 | 0.5875 | 0.6641 |\n",
        "| RandomForest | only_comment_ratio | 1 | 0.6804 | 0.6593 |\n",
        "| XGBoost | only_comment_ratio | 1 | 0.6804 | 0.6574 |\n",
        "| MLP | only_comment_ratio | 1 | 0.6803 | 0.6532 |\n",
        "| DeepNN | only_comment_ratio | 1 | 0.6798 | 0.6508 |\n",
        "| LogReg | only_comment_ratio | 1 | 0.6735 | 0.6497 |\n",
        "| LogReg | prev_2feats | 2 | 0.6803 | 0.6464 |\n",
        "| XGBoost | prev_2feats | 2 | 0.6922 | 0.6455 |\n",
        "| MLP | prev_2feats | 2 | 0.6916 | 0.6447 |\n",
        "| RandomForest | prev_2feats | 2 | 0.6921 | 0.6441 |\n",
        "| DeepNN | prev_2feats | 2 | 0.6911 | 0.6421 |\n",
        "| DeepNN | drop_error_near_eof_ratio | 10 | 0.7496 | 0.6237 |\n",
        "| XGBoost | drop_error_near_eof_ratio | 10 | 0.7511 | 0.6213 |\n",
        "| LogReg | drop_error_near_eof_ratio | 10 | 0.7341 | 0.6209 |\n",
        "| RandomForest | drop_error_near_eof_ratio | 10 | 0.7509 | 0.6201 |\n",
        "| MLP | drop_error_near_eof_ratio | 10 | 0.7503 | 0.6198 |\n",
        "| MultinomialNB | drop_error_near_eof_ratio | 10 | 0.7241 | 0.6089 |\n",
        "| XGBoost | only_comments_code_like_ratio_to_total | 1 | 0.5733 | 0.6005 |\n",
        "| LogReg | ALL | 11 | 0.7618 | 0.6003 |\n",
        "| RandomForest | only_comments_code_like_ratio_to_total | 1 | 0.5730 | 0.5991 |\n",
        "| DeepNN | only_comments_code_like_ratio_to_total | 1 | 0.5721 | 0.5981 |\n",
        "| MLP | only_comments_code_like_ratio_to_total | 1 | 0.5726 | 0.5973 |\n",
        "| LogReg | only_comments_code_like_ratio_to_total | 1 | 0.5725 | 0.5969 |\n",
        "| MultinomialNB | drop_text_like_ratio | 10 | 0.7350 | 0.5956 |\n",
        "| MultinomialNB | ALL | 11 | 0.7487 | 0.5942 |\n",
        "\n",
        "*Full results (126 experiments) exported to `data/results_all_models.xlsx`*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| only_comments_text_like_ratio_to_total | 1 | 0.6131 | **0.6900** |\n",
        "| only_verb_ratio_comments | 1 | 0.5877 | 0.6658 |\n",
        "| only_comment_ratio | 1 | 0.6735 | 0.6497 |\n",
        "| prev_2feats | 2 | 0.6803 | 0.6464 |\n",
        "| drop_error_near_eof_ratio | 10 | 0.7341 | 0.6209 |\n",
        "| ALL | 11 | 0.7618 | 0.6003 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7618 | 0.6003 |\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7618 | 0.6003 |\n",
        "| drop_bucket_medium | 10 | 0.7617 | 0.6003 |\n",
        "| drop_bucket_small | 10 | 0.7618 | 0.6003 |\n",
        "| drop_text_like_ratio | 10 | 0.7487 | 0.6002 |\n",
        "| drop_bucket_large | 10 | 0.7618 | 0.6002 |\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7623 | 0.5989 |\n",
        "| only_comments_code_like_ratio_to_total | 1 | 0.5725 | 0.5969 |\n",
        "| drop_verb_ratio_comments | 10 | 0.7621 | 0.5956 |\n",
        "| drop_comment_ratio | 10 | 0.7577 | 0.5952 |\n",
        "| only_comments_code_like_ratio_comments | 1 | 0.5712 | 0.5938 |\n",
        "| only_comments_text_like_ratio_comments | 1 | 0.5712 | 0.5938 |\n",
        "| drop_comments_text_like_ratio_to_total | 10 | 0.7584 | 0.5869 |\n",
        "| only_text_like_ratio | 1 | 0.6785 | 0.5346 |\n",
        "| only_error_near_eof_ratio | 1 | 0.5907 | 0.4074 |\n",
        "\n",
        "**Best Configuration:** `only_comments_text_like_ratio_to_total` with Test F1 = 0.6900"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| only_comments_text_like_ratio_to_total | 1 | 0.6139 | **0.6932** |\n",
        "| only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| only_comment_ratio | 1 | 0.6804 | 0.6593 |\n",
        "| prev_2feats | 2 | 0.6922 | 0.6452 |\n",
        "| drop_error_near_eof_ratio | 10 | 0.7513 | 0.6198 |\n",
        "| only_comments_code_like_ratio_to_total | 1 | 0.5731 | 0.6025 |\n",
        "| only_comments_code_like_ratio_comments | 1 | 0.5716 | 0.5960 |\n",
        "| only_comments_text_like_ratio_comments | 1 | 0.5716 | 0.5960 |\n",
        "| drop_text_like_ratio | 10 | 0.7747 | 0.5896 |\n",
        "| drop_comment_ratio | 10 | 0.7800 | 0.5887 |\n",
        "| drop_bucket_large | 10 | 0.7892 | 0.5877 |\n",
        "| ALL | 11 | 0.7893 | 0.5829 |\n",
        "| drop_verb_ratio_comments | 10 | 0.7890 | 0.5826 |\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7880 | 0.5824 |\n",
        "| drop_bucket_medium | 10 | 0.7893 | 0.5821 |\n",
        "| drop_bucket_small | 10 | 0.7892 | 0.5815 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7893 | 0.5796 |\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7893 | 0.5796 |\n",
        "| only_text_like_ratio | 1 | 0.6983 | 0.5767 |\n",
        "| drop_comments_text_like_ratio_to_total | 10 | 0.7878 | 0.5689 |\n",
        "| only_error_near_eof_ratio | 1 | 0.5940 | 0.4066 |\n",
        "\n",
        "**Best Configuration:** `only_comments_text_like_ratio_to_total` with Test F1 = 0.6932"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| only_comments_text_like_ratio_to_total | 1 | 0.6138 | **0.6930** |\n",
        "| only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| only_comment_ratio | 1 | 0.6804 | 0.6574 |\n",
        "| prev_2feats | 2 | 0.6922 | 0.6455 |\n",
        "| drop_error_near_eof_ratio | 10 | 0.7511 | 0.6213 |\n",
        "| only_comments_code_like_ratio_to_total | 1 | 0.5733 | 0.6005 |\n",
        "| only_comments_code_like_ratio_comments | 1 | 0.5717 | 0.5960 |\n",
        "| only_comments_text_like_ratio_comments | 1 | 0.5717 | 0.5960 |\n",
        "| drop_text_like_ratio | 10 | 0.7745 | 0.5901 |\n",
        "| drop_bucket_large | 10 | 0.7886 | 0.5875 |\n",
        "| drop_bucket_small | 10 | 0.7887 | 0.5868 |\n",
        "| drop_comment_ratio | 10 | 0.7796 | 0.5863 |\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7878 | 0.5837 |\n",
        "| drop_verb_ratio_comments | 10 | 0.7887 | 0.5829 |\n",
        "| drop_bucket_medium | 10 | 0.7888 | 0.5820 |\n",
        "| drop_comments_text_like_ratio_to_total | 10 | 0.7875 | 0.5813 |\n",
        "| ALL | 11 | 0.7886 | 0.5809 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7886 | 0.5809 |\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7886 | 0.5809 |\n",
        "| only_text_like_ratio | 1 | 0.6983 | 0.5750 |\n",
        "| only_error_near_eof_ratio | 1 | 0.5939 | 0.4066 |\n",
        "\n",
        "**Best Configuration:** `only_comments_text_like_ratio_to_total` with Test F1 = 0.6930"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP (Multi-Layer Perceptron) Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| only_comments_text_like_ratio_to_total | 1 | 0.6133 | **0.6906** |\n",
        "| only_verb_ratio_comments | 1 | 0.5878 | 0.6657 |\n",
        "| only_comment_ratio | 1 | 0.6803 | 0.6532 |\n",
        "| prev_2feats | 2 | 0.6916 | 0.6447 |\n",
        "| drop_error_near_eof_ratio | 10 | 0.7499 | 0.6136 |\n",
        "| only_comments_code_like_ratio_to_total | 1 | 0.5726 | 0.5973 |\n",
        "| only_comments_code_like_ratio_comments | 1 | 0.5716 | 0.5960 |\n",
        "| only_comments_text_like_ratio_comments | 1 | 0.5716 | 0.5960 |\n",
        "| drop_text_like_ratio | 10 | 0.7739 | 0.5881 |\n",
        "| drop_comment_ratio | 10 | 0.7789 | 0.5847 |\n",
        "| drop_bucket_medium | 10 | 0.7885 | 0.5830 |\n",
        "| ALL | 11 | 0.7881 | 0.5819 |\n",
        "| drop_bucket_small | 10 | 0.7882 | 0.5776 |\n",
        "| drop_verb_ratio_comments | 10 | 0.7880 | 0.5768 |\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7868 | 0.5752 |\n",
        "| only_text_like_ratio | 1 | 0.6971 | 0.5739 |\n",
        "| drop_bucket_large | 10 | 0.7880 | 0.5705 |\n",
        "| drop_comments_text_like_ratio_to_total | 10 | 0.7867 | 0.5697 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7883 | 0.5680 |\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7883 | 0.5680 |\n",
        "| only_error_near_eof_ratio | 1 | 0.5940 | 0.4066 |\n",
        "\n",
        "**Best Configuration:** `only_comments_text_like_ratio_to_total` with Test F1 = 0.6906"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Neural Network Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| only_comments_text_like_ratio_to_total | 1 | 0.6129 | **0.6878** |\n",
        "| only_verb_ratio_comments | 1 | 0.5875 | 0.6641 |\n",
        "| only_comment_ratio | 1 | 0.6798 | 0.6508 |\n",
        "| prev_2feats | 2 | 0.6911 | 0.6421 |\n",
        "| drop_error_near_eof_ratio | 10 | 0.7496 | 0.6237 |\n",
        "| only_comments_code_like_ratio_to_total | 1 | 0.5721 | 0.5981 |\n",
        "| only_comments_code_like_ratio_comments | 1 | 0.5711 | 0.5948 |\n",
        "| only_comments_text_like_ratio_comments | 1 | 0.5711 | 0.5948 |\n",
        "| ALL | 11 | 0.7872 | 0.5907 |\n",
        "| drop_text_like_ratio | 10 | 0.7738 | 0.5896 |\n",
        "| drop_bucket_small | 10 | 0.7887 | 0.5868 |\n",
        "| drop_bucket_large | 10 | 0.7879 | 0.5862 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7876 | 0.5843 |\n",
        "\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7876 | 0.5843 |**Best Configuration:** `only_comments_text_like_ratio_to_total` with Test F1 = 0.6878\n",
        "\n",
        "| drop_bucket_medium | 10 | 0.7881 | 0.5835 |\n",
        "\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7863 | 0.5831 || only_error_near_eof_ratio | 1 | 0.5938 | 0.4058 |\n",
        "\n",
        "| drop_comment_ratio | 10 | 0.7787 | 0.5805 || drop_verb_ratio_comments | 10 | 0.7874 | 0.5568 |\n",
        "\n",
        "| only_text_like_ratio | 1 | 0.6965 | 0.5721 || drop_comments_text_like_ratio_to_total | 10 | 0.7860 | 0.5717 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multinomial Naive Bayes Results\n",
        "\n",
        "| Features | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|----------|---------|-------------|---------------|\n",
        "| drop_error_near_eof_ratio | 10 | 0.7241 | **0.6089** |\n",
        "| drop_text_like_ratio | 10 | 0.7350 | 0.5956 |\n",
        "| ALL | 11 | 0.7487 | 0.5942 |\n",
        "| drop_comment_ratio | 10 | 0.7429 | 0.5923 |\n",
        "| drop_comments_code_like_ratio_to_total | 10 | 0.7492 | 0.5923 |\n",
        "| drop_verb_ratio_comments | 10 | 0.7467 | 0.5916 |\n",
        "| drop_bucket_large | 10 | 0.7540 | 0.5908 |\n",
        "| drop_comments_text_like_ratio_to_total | 10 | 0.7413 | 0.5907 |\n",
        "| drop_comments_text_like_ratio_comments | 10 | 0.7455 | 0.5775 |\n",
        "| drop_comments_code_like_ratio_comments | 10 | 0.7455 | 0.5775 |\n",
        "| drop_bucket_small | 10 | 0.7057 | 0.5744 |\n",
        "| drop_bucket_medium | 10 | 0.7345 | 0.5682 |\n",
        "| prev_2feats | 2 | 0.5696 | 0.5208 |\n",
        "| only_* (all single features) | 1 | 0.3435 | 0.2311 |\n",
        "\n",
        "**Best Configuration:** `drop_error_near_eof_ratio` with Test F1 = 0.6089\n",
        "\n",
        "**Note:** MultinomialNB requires non-negative features (MinMaxScaler applied). Single-feature configurations perform extremely poorly (F1 ~ 0.23) due to the scaler normalizing all values to a narrow range, losing discriminative power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Configuration Per Model Summary\n",
        "\n",
        "| Model | Best Feature Subset | N_feats | CV_F1_macro | Test_F1_macro |\n",
        "|-------|---------------------|---------|-------------|---------------|\n",
        "| RandomForest | only_comments_text_like_ratio_to_total | 1 | 0.6139 | **0.6932** |\n",
        "| XGBoost | only_comments_text_like_ratio_to_total | 1 | 0.6138 | 0.6930 |\n",
        "| Stacking Ensemble | only_comments_text_like_ratio_to_total | 1 | - | 0.6911 |\n",
        "| MLP | only_comments_text_like_ratio_to_total | 1 | 0.6133 | 0.6906 |\n",
        "| LogReg | only_comments_text_like_ratio_to_total | 1 | 0.6131 | 0.6900 |\n",
        "| DeepNN | only_comments_text_like_ratio_to_total | 1 | 0.6129 | 0.6878 |\n",
        "| MultinomialNB | drop_error_near_eof_ratio | 10 | 0.7241 | 0.6089 |\n",
        "\n",
        "**Notes:**\n",
        "- Stacking Ensemble uses RF, GradientBoosting, MLP, LogReg, and ExtraTrees as base models with LogReg meta-classifier\n",
        "- MultinomialNB performs poorly with single features due to MinMaxScaler requirements; best with 10 features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation & Key Findings\n",
        "\n",
        "### 1. Severe Overfitting with Multi-Feature Models\n",
        "- **All models with 10-11 features show strong overfitting**: CV F1 scores of 0.75-0.79 but Test F1 only 0.55-0.62\n",
        "- The gap between CV and Test performance (~0.15-0.20) indicates the models memorize training patterns that don't generalize\n",
        "\n",
        "### 2. Single Feature Dominance\n",
        "The best performing configuration across **all models** is using **only `comments_text_like_ratio_to_total`**:\n",
        "- Achieves Test F1 = **0.69-0.70** with just 1 feature\n",
        "- This significantly outperforms using all 11 features (Test F1 ~ 0.58-0.60)\n",
        "- **Simpler models generalize better** for this task\n",
        "\n",
        "### 3. Top Individual Features (Ranked by Test F1)\n",
        "| Rank | Feature | Test F1 |\n",
        "|------|---------|---------|\n",
        "| 1 | `comments_text_like_ratio_to_total` | 0.69-0.70 |\n",
        "| 2 | `verb_ratio_comments` | 0.66-0.67 |\n",
        "| 3 | `comment_ratio` | 0.65-0.66 |\n",
        "| 4 | `comments_code_like_ratio_to_total` | 0.60 |\n",
        "| 5 | `comments_code_like_ratio_comments` | 0.59-0.60 |\n",
        "\n",
        "### 4. Harmful Feature: `error_near_eof_ratio`\n",
        "- Using **only** `error_near_eof_ratio` gives worst Test F1 (0.40-0.41)\n",
        "- **Dropping** this feature consistently improves test performance:\n",
        "  - LogReg: 0.6003 -> 0.6209 (+0.02)\n",
        "  - RandomForest: 0.5829 -> 0.6201 (+0.04)\n",
        "  - XGBoost: 0.5809 -> 0.6213 (+0.04)\n",
        "  - MultinomialNB: 0.5942 -> 0.6089 (+0.01)\n",
        "- This feature appears to introduce spurious correlations that hurt generalization\n",
        "\n",
        "### 5. Model Complexity vs. Performance\n",
        "| Complexity | Model | Best Test F1 |\n",
        "|------------|-------|--------------|\n",
        "| Low | LogReg | 0.6900 |\n",
        "| Medium | RandomForest | **0.6932** |\n",
        "| Medium | XGBoost | 0.6930 |\n",
        "| Medium | Stacking Ensemble | 0.6911 |\n",
        "| High | MLP | 0.6906 |\n",
        "| Very High | DeepNN | 0.6878 |\n",
        "| Naive Bayes | MultinomialNB | 0.6089 |\n",
        "\n",
        "- **Random Forest achieves the overall best** Test F1 (0.6932)\n",
        "- Simpler models (LogReg, RF, XGBoost) outperform complex neural networks\n",
        "- **Stacking Ensemble provides no improvement** over individual models (0.6911 vs 0.6932)\n",
        "- MultinomialNB struggles with single features but achieves 0.61 with 10 features\n",
        "\n",
        "### 6. Stacking Ensemble Analysis\n",
        "- Ensemble of RF, GradientBoosting, MLP, LogReg, ExtraTrees with LogReg meta-classifier\n",
        "- **Test F1: 0.6911** (trained on 30k subset)\n",
        "- Delta vs best single model: -0.0021 (no gain)\n",
        "- The ensemble fails to improve because all base models converge to similar predictions on this single-feature task\n",
        "\n",
        "### 7. Recommendations\n",
        "1. **Use `comments_text_like_ratio_to_total` as the primary feature** - it alone provides ~69% F1\n",
        "2. **Remove `error_near_eof_ratio`** from feature sets - it hurts generalization\n",
        "3. **Prefer simpler models** (Logistic Regression or Random Forest) over deep networks\n",
        "4. If combining features, use only 2-3: `comments_text_like_ratio_to_total`, `verb_ratio_comments`, and optionally `comment_ratio`\n",
        "5. The `prev_2feats` combination (verb_ratio_comments + comment_ratio) gives stable ~64.5% F1 across models\n",
        "6. **Skip ensemble methods** - they add complexity without improving performance\n",
        "\n",
        "### 8. Distribution Shift Analysis\n",
        "The large CV-to-Test gap suggests significant **distribution shift** between:\n",
        "- Training data (300K samples)\n",
        "- Test-Add evaluation set (165K samples)\n",
        "\n",
        "Features that capture comment structure (`comments_text_like_ratio_to_total`) appear more robust to this shift than syntactic features or error patterns."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
